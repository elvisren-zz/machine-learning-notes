\section{Inner Product Space}


\subsection{Inner Product and Norm}

\begin{definition}
	An \cindex{inner product} on $V$ is a function $V \rightarrow V \rightarrow F$ ($F$ is either $C$ or $R$) that $\forall x,y,z \in V$ and $\forall c \in F$ that:
	\begin{enumerate}
		\item $\innerproduct{x+z}{y} = \innerproduct{x}{y} + \innerproduct{z}{y}$ \label{firstproductdefinition}
		\item $\innerproduct{cx}{y} = c \innerproduct{x}{y}$ \label{secondproductdefinition}
		\item $\overline{\innerproduct{x}{y}} = \innerproduct{y}{x}$
		\item $\innerproduct{x}{x} > 0$
	\end{enumerate}
	Item (\ref{firstproductdefinition}) and (\ref{secondproductdefinition}) means the inner product is \emph{linear in first component}.
	Please be noted that the result of inner product could be a complex value.
	\qed
\end{definition}


\begin{theorem}
	properties of inner product:
	\begin{enumerate}
		\item $\innerproduct{x}{y+z} = \innerproduct{x}{y} + \innerproduct{x}{z}$ \label{firstproductproperty}
		\item $\innerproduct{x}{cy} = \overline{c} \innerproduct{x}{y} $ \label{secondproductproperty}
		\item $\innerproduct{x}{x} = 0 \iff x = 0$
		\item If $\innerproduct{x}{y} = \innerproduct{x}{z}$ for all $x \in V$, then $y=z$.
	\end{enumerate}
	Item (\ref{firstproductproperty}) and (\ref{secondproductproperty}) means the inner product is \emph{conjugate linear in second component}.
\end{theorem}


\begin{definition}
	the \cindex{standard inner product} on $F^n$ for $x=(a_1,a_2,\dots,a_n)$ and $y=(b_1,b_2,\dots,b_n)$ is:
	\begin{equation}
		\innerproduct{x}{y} = \sum_{i=1}^n a_i \overline{b_i}		
	\end{equation}
	when $F=R$, it is usually called \cindex{dot product} and denoted as $x \cdot y$.
\end{definition}

\begin{definition}
	For $A \in M_{m \times n}(F)$, the \cindex{conjugate transpose} or \cindex{adjoint} of $A$ is $A^* \in M_{n \times m}(F)$ that $(A^*)_{ij} = \overline{A_{ji}}$. If $A$ is complex, $A^* = \overline{A^\top}$ .If $A$ is real, $A^*$ is $A^\top$.
\end{definition}

\begin{definition}[Forbenius Inner Product]
    Let $V=M_{n \times n} (F)$, the \cindex{Forbenius Inner Product} is defined as:
    \begin{equation}
        \innerproduct{A}{B} = \text{tr}(B^* A)
    \end{equation}
\end{definition}

\begin{definition}\label{hinnerproductspace}
	The continuous complex-valued function on interval $[0, 2\pi]$ is a inner product space $H$:
	\begin{equation}
		\innerproduct{f}{g} = \frac{1}{2\pi} \int_{0}^{2\pi} f(t) \overline{g(t)} dt
	\end{equation}
\end{definition}


\begin{definition}
	the \cindex{norm} or \cindex{length} of $x$ is:
	\begin{equation}
	    \norm{x} = \sqrt{\innerproduct{x}{x}}
	\end{equation}
\end{definition}

\begin{theorem}
	the property of norm:
	\begin{itemize}
		\item $\norm{cx} = \absolutevalue{c} \norm{x}$
		\item $\norm{x} = 0 \iff x = 0$
		\item \cindex{Cauchy-Schwarz Inequality} $\absolutevalue{\innerproduct{x}{y}} \leq \norm{x} \cdot \norm{y}$
		\item \cindex{Triangle Inequality} $\norm{x+y} \leq \norm{x} + \norm{y}$
	\end{itemize}
\end{theorem}






% orthogonal

\subsection{Orthogonal and Gram-Schmidt Process}

\begin{definition}
	$x$ and $y$ are \cindex{orthogonal} if $\innerproduct{x}{y} = 0$. A subset $S$ of $V$ is orthogonal if any two vectors in $S$ are orthogonal. A subset $S$ of $V$ is \cindex{orthonormal} if $S$ is orthogonal and consists entirely of unit vectors.
\end{definition}

\begin{definition}
    \begin{equation}
        \innerproduct{x}{y} = \norm{x} \cdot \norm{y} \text{cos}(\theta)
    \end{equation}    
\end{definition}


\begin{definition}
	A vector is \cindex{unit vector} if $\norm{x} = 1$. A \cindex{normalizing} to non-zero $x$ is $\dfrac{1}{\norm{x}} x$.
\end{definition}


\begin{theorem}
    Let $f_n (t) = e^{i nt}$ where $0 \leq t \leq 2 \pi$. All $f_i$ are orthogonal.
\end{theorem}
\begin{proof}
    \begin{equation}
        \begin{aligned}
            \innerproduct{f_m}{f_n} &= \frac{1}{2 \pi} \int_0^{2 \pi} e^{imt} \overline{e^{int}} \dif{t} \\
            &= \frac{1}{2\pi} \int_0^{2\pi} e^{i (m-n) t} \dif{t} \\
            &= \left. \frac{1}{2\pi (m-n)} e^{i(m-n)t} \right|_0^{2\pi} \\
            &= 0
        \end{aligned}
    \end{equation}
\end{proof}



\begin{definition}
	A \cindex{orthonormal basis} for $V$ is an ordered basis that is orthonormal.
\end{definition}

\begin{theorem}
	let $S=\{ v_1, v_2, \dots, v_k \}$ be an orthogonal subset of $V$ consisting of non-zero vectors. If $y \in \text{span}(S)$, then
	\begin{equation}
		y = \sum_{i=1}^k \frac{\innerproduct{y}{v_i}}{\norm{v_i}^2} v_i
	\end{equation}
	If $S$ is orthonormal, then
	\begin{equation}
		y = \sum_{i=1}^k \innerproduct{y}{v_i} v_i
	\end{equation}
\end{theorem}
\begin{proof}
	let $\displaystyle y = \sum_{i=1}^k a_i v_i$. we have
	\begin{equation*}
		\innerproduct{y}{v_i} = \innerproduct{\sum_{i=1}^k a_i v_i}{v_j} = \sum_{i=1}^k a_i \innerproduct{v_i}{v_j} = a_j \norm{v_j}^2
	\end{equation*}
	So $\displaystyle a_j = \frac{\innerproduct{y}{v_j}}{\norm{v_j}^2}$.
	
	
\end{proof}

\begin{theorem}
	an orthogonal subset of $V$ is linearly independent.
\end{theorem}

\begin{definition}[\cindex{Gram-Schmidt process}]
	Let $S=\{w_1, w_2, \dots, w_n \}$ be linearly independent subset of $V$. Define $S^\prime=\{v_1,v_2,\dots,v_n  \}$, where $v_1=w_1$ and 
	\begin{equation}
		v_k = w_k - \sum_{j=1}^{k-1} \frac{\innerproduct{w_k}{v_j}}{\norm{v_j}^2} v_j
	\end{equation}
	then $S^\prime$ is an orthogonal set of non-zero vectors that $\text{span}(S^\prime) = \text{span}(S)$. The process is that for the $k$th basis $w_k$, first project it on top of the $k-1$ orthogonal vectors $\displaystyle \sum_{j=1}^{k-1} \frac{\innerproduct{w_k}{v_j}}{\norm{v_j}^2} v_j$, and calculate the reciprocal vector $\displaystyle w_k - \sum_{j=1}^{k-1} \frac{\innerproduct{w_k}{v_j}}{\norm{v_j}^2} v_j$.
	\qed
\end{definition}

\begin{theorem}\label{vectorinorthonormalbasis}
	If $V$ has an orthonormal basis $\beta=\{v_1,v_2,\dots,v_n\}$, then $\forall x\in V$, 
	\begin{equation}
		x = \sum_{i=1}^n \innerproduct{x}{v_j} v_i
	\end{equation}
\end{theorem}

\begin{definition}
    Let $\beta$ be an orthonormal subset of $V$. For $x \in V$, the \cindex{Fourier coefficients} of $x$ relative to $\beta$ are $\innerproduct{x}{y_i}$ for all $y_i \in \beta$.
\end{definition}


\begin{theorem}
	Let $V$ with an orthonormal basis $\beta=\{v_1,v_2,\dots,v_n\}$. $T$ is a linear operator on $V$ and let $A=[T]_\beta$. then $A_{ij}=\innerproduct{T(v_j)}{v_i}$.
\end{theorem}
\begin{proof}
	From theorem (\ref{vectorinorthonormalbasis}) we have
	\begin{equation*}
		T(v_j) = \sum_{i=1}^n \innerproduct{T(v_j)}{v_i} v_i
	\end{equation*}
\end{proof}

\begin{definition}
    Let $S$ be nonempty subset of $V$. The \cindex{orthogonal complement} of $S$ is $S^\bot$ that $\forall x \in S, \forall y \in S^\bot, \innerproduct{x}{y} = 0$.
\end{definition}

\begin{theorem}\index{orthogonalprojection}
    Let $W$ be a subspace of $V$. For $y \in V$, there is unique $u \in W$ and $z \in W^\bot$ that $y = u + z$. $u$ is the \cindex{orthogonal projection} of $y$ on $W$. If $\{v_1, v_2, \dots, v_k \}$ is an orthonormal basis of $W$, then 
    \begin{equation}
        \begin{aligned}
            u &= \sum_{i=1}^k \innerproduct{y}{v_i} v_i \\
            z &= y -   \sum_{i=1}^k \innerproduct{y}{v_i} v_i          
        \end{aligned}
    \end{equation}
\end{theorem}

\begin{theorem}
    Let $y$,$u$,$z$ as defined in Theorem (\ref{orthogonalprojection}). $u$ is the closest vector in $W$ to $y$ that is $\forall x \in W$, $\norm{y-x} \geq \norm{y - u}$.
\end{theorem}
\begin{proof}
    \begin{equation*}
        \begin{aligned}
            \norm{y - x}^2 &= \norm{u + z - x}^2 \\
            &= \norm{(u - x) + z}^2 \\
            &= \norm{u - x}^2 + \norm{z}^2 \\
            &\geq \norm{z}^2 = \norm{y - u}^2
        \end{aligned}
    \end{equation*}
\end{proof}



\begin{theorem}
    For $S=\{v_1, v_2, \dots, v_k \}$ be an orthogonal subset of $V$. For $\forall y \in V$, the projection of $y$ on $S$ is $\displaystyle u = \sum_{i=1}^k \frac{\innerproduct{y}{v_i}}{\norm{v_i}^2} v_i$. If $S$ are orthonormal, $\displaystyle u = \sum_{i=1}^k \innerproduct{y}{v_i} v_i$. If $y$ is in span of $S$, then $y = u$.
\end{theorem}



% Adjoint of Linear Operator
\subsection{Adjoint of Linear Operator}

\begin{theorem}\label{uniquelinearoperatortof}
    Let $g: V \rightarrow F$ be a linear transformation. Then there exist a unique $y \in V$ that $\forall x \in V$, $g(x) = \innerproduct{x}{y}$. The $y$ is 
    \begin{equation}
        y = \sum_{i=1}^n \overline{g(v_i)} v_i
    \end{equation}
\end{theorem}

\begin{theorem}
    Let $T$ be a linear operator on $V$. Then there existing a unique linear operator $T^* : V \rightarrow V$ that $\innerproduct{T(x)}{y}=\innerproduct{x}{T^*(y)}$ for all $x,y \in V$. $T^*$ is called the \cindex{adjoint} of $T$.
\end{theorem}
\begin{proof}
    For each $y$, $\innerproduct{T(x)}{y}$ is a linear operator from $V$ to $F$, so by Theorem (\ref{uniquelinearoperatortof}) $\exists y'$ that $\innerproduct{T(x)}{y} = \innerproduct{x}{y'}$.
\end{proof}

\begin{theorem}
    \begin{equation}
        \innerproduct{x}{T(y)} = \innerproduct{T^*(x)}{y}
    \end{equation}
    So a $^*$ is added to $T$ when change the location of $T$.
\end{theorem}
\begin{proof}
    \begin{equation*}
        \innerproduct{x}{T(y)} = \overline{\innerproduct{T(y)}{x}} = \overline{\innerproduct{y}{T^*(x)}} = \innerproduct{T^*(x)}{y}
    \end{equation*}
\end{proof}

\begin{theorem}
    Let $\beta$ be a orthonormal basis for $V$. If $T$ is a linear operation on $V$ then
    \begin{equation}
        [T^*]_\beta = ([T]_\beta)^*
    \end{equation}
    Let $A$ be an $n \times n$ matrix. Then
    \begin{equation}
        L_{A^*} = (L_A)^*
    \end{equation}
\end{theorem}
\begin{proof}
    Let $A=[T]_\beta$, $B=([T^*])_\beta$, and $\beta=\{v_1, v_2, \dots, v_n \}$. Then
    \begin{equation*}
        B_{ij} = \innerproduct{T^*(v_j)}{v_i} = \overline{\innerproduct{v_i}{T^*(v_j)}} = \overline{\innerproduct{T(v_i)}{v_j}} = \overline{A_{ji}} = (A^*)_{ij}
    \end{equation*}
\end{proof}

\begin{theorem}
    Let $T$ and $U$ be linear operator on $V$, then
    \begin{enumerate}
        \item $(T+U)^* = T^* + U^*$
        \item $(cT)^* = \overline{c} T^*$
        \item $(UT)^* = T^* U^*$
        \item $T^{**} = T$
    \end{enumerate}    
\end{theorem}


% Example in statistics
\subsection{Examples in Statistics}

% Least Square Approximation
\subsubsection{Least Square Approximation}


\begin{definition}
    The \cindex{Least Square Approximation} is a problem that for $A = \begin{pmatrix}
        t_1 & 1 \\
        t_2 & 1 \\
        \vdots & \vdots \\
        t_m & 1
    \end{pmatrix}$,  $y = \begin{pmatrix}
        y_1\\
        y_2\\
        \vdots\\
        y_m
    \end{pmatrix}$, find $x_0 = \begin{pmatrix}
        c \\
        d
    \end{pmatrix}$ that minimize $\norm{Ax-y}$.
\end{definition}

\begin{definition}
    For $x,y \in F^n$, define $\innerproduct{x}{y}_n = y^* \times x$.
\end{definition}


\begin{theorem}
    Let $A \in M_{m \times n} (F)$, $x \in F^n$, $y\in F^m$, then
    \begin{equation}
        \innerproduct{Ax}{y}_m = \innerproduct{x}{A^* y}_n
    \end{equation}
\end{theorem}
\begin{proof}
    $\innerproduct{Ax}{y}_m = y^* \times (Ax) = (y^* \times A) x = (A^* y)^* x = \innerproduct{x}{A^* y}_n$
\end{proof}

\begin{theorem}
    Let $A \in M_{m\times n} (F)$. Then
    \begin{equation}
        \rank{A^*A} = \rank{A}
    \end{equation}
    So if $\rank{A} = n$, $A^*A$ is invertible.
\end{theorem}
\begin{proof}
    For equation $A^*Ax = 0$ and $Ax = 0$. $Ax=0$ implies that $A^*Ax =0$. Then assume $A^*Ax = 0$, then
    \begin{equation*}
        0 = \innerproduct{0}{x}_n = \innerproduct{A^*Ax}{x}_n = \innerproduct{Ax}{A^{**}x}_m = \innerproduct{Ax}{Ax}_m
    \end{equation*}
\end{proof}


\begin{theorem}
    Let $A \in M_{m\times n} (F)$, $y \in F^m$. Then there exists $x_0 \in F^n$ that $(A^*A) x_0 = A^* y$ and $\forall x \in F^n$, $ \norm{Ax_0 - y} \leq \norm{Ax-y}$. If $\rank{A} = n$, then $x_0 = (A^*A)^{-1} A^* y$.
\end{theorem}
\begin{proof}
    Define $W=R(L_A)$. There exists a $x_0$ that is closest to $y$ that $Ax_0 - y \in W^\bot$, so $\innerproduct{Ax}{Ax_0 - y}_m = 0$. So $\innerproduct{x}{A^*(Ax_0 - y)}_n = 0$, so $A^*(Ax_0 - y) = 0$ and $(A^*A) x_0 = A^* y$. 
\end{proof}

% Minimal Solution to Linear Equations
\subsubsection{Minimal Solution to Linear Equations}

\begin{definition}
    A solution $s$ to $Ax=b$ is \cindex{minimal solution} if $\norm{s} \leq \norm{u}$ for all other solution $u$.
\end{definition}

\begin{theorem}
    If $V$ is finite dimentional, let $T$ be a linear operator on $V$, then
    \begin{equation*}
        \begin{aligned}
            R(T^*)^\bot &= N(T)\\
            R(T^*) &= N(T)^\bot
        \end{aligned}
    \end{equation*}    
\end{theorem}
\begin{proof}
    If $m \in R(T^*)^\bot$, $\forall x \in V$, $0 = \innerproduct{m}{T^*x} = \innerproduct{T(m)}{x}$, so $m \in N(T)$.
\end{proof}


\begin{theorem}
    Let $A \in M_{m\times n} (F)$, $y \in F^m$. Suppose $Ax=y$ is consistent. Then there exists unique minimal solution $s \in R(L_{A^*})$ of $Ax=y$. And $s$ is the only solution in $R(L_{A^*})$. If $u$ is a solution to $(AA^*) u = y$, then $s = A^* u$.
\end{theorem}
\begin{proof}
    Define $W = R(L_{A^*})$ and $W^\bot = N(L_A)$. $\forall x$ that $Ax = y$, we have $s \in W$ and $t \in W^\bot$ that $x=s+t$. So $y = Ax = A(s + t) = As + At = As$. So $s$ is a solution to $Ax=y$. From Theorem (\ref{equationfromoneandnullspace}), all solution to $Ax=y$ has the form $x' = s + t'$ where $t' \in W^\bot$. And $\norm{x'}^2 = \norm{s + t'}^2 = \norm{s}^2 + \norm{t'}^2 \geq \norm{s}^2$.
\end{proof}










