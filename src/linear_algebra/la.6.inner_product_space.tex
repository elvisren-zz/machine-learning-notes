\section{Inner Product Space}


\subsection{Inner Product and Norm}

\begin{definition}
	An \cindex{inner product} on $V$ is a function $V \rightarrow V \rightarrow F$ ($F$ is either $C$ or $R$) that $\forall x,y,z \in V$ and $\forall c \in F$ that:
	\begin{enumerate}
		\item $\innerproduct{x+z}{y} = \innerproduct{x}{y} + \innerproduct{z}{y}$ \label{firstproductdefinition}
		\item $\innerproduct{cx}{y} = c \innerproduct{x}{y}$ \label{secondproductdefinition}
		\item $\overline{\innerproduct{x}{y}} = \innerproduct{y}{x}$
		\item $\innerproduct{x}{x} > 0$ if $x \neq 0$
	\end{enumerate}
	Item (\ref{firstproductdefinition}) and (\ref{secondproductdefinition}) means the inner product is \emph{linear in first component}.
	Please be noted that the result of inner product could be a complex value, but the result of $\innerproduct{x}{x}$ is a non-negative real number.
	\qed
\end{definition}


\begin{theorem}
	properties of inner product:
	\begin{enumerate}
		\item $\innerproduct{x}{y+z} = \innerproduct{x}{y} + \innerproduct{x}{z}$ \label{firstproductproperty}
		\item $\innerproduct{x}{cy} = \overline{c} \innerproduct{x}{y} $ \label{secondproductproperty}
		\item $\innerproduct{x}{x} = 0$ if and only if $x = 0$.
		\item If $\innerproduct{x}{y} = \innerproduct{x}{z}$ for all $x \in V$, then $y=z$.
	\end{enumerate}
	Item (\ref{firstproductproperty}) and (\ref{secondproductproperty}) means the inner product is \cindex{conjugate linear} in second component.
\end{theorem}


\begin{definition}
	the \cindex{standard inner product} on $F^n$ for $x=\rowvector{a_1,a_2,\dots,a_n}$ and $y=\rowvector{b_1,b_2,\dots,b_n}$ is:
	\begin{equation}
		\innerproduct{x}{y} = \sum_{i=1}^n a_i \overline{b_i}		
	\end{equation}
	when $F=R$, it is usually called \cindex{dot product} and denoted as $x \cdot y$.
\end{definition}

\begin{definition}
	For $A \in M_{m \times n}(F)$, the \cindex{conjugate transpose} or \cindex{adjoint} of $A$ is $A^* \in M_{n \times m}(F)$ that $(A^*)_{ij} = \overline{A_{ji}}$. If $A$ is complex, $A^* = \overline{A^\top}$ . If $A$ is real, $A^*$ is $A^\top$.
\end{definition}

\begin{definition}[Forbenius Inner Product]
    Let $V=M_{n \times n} (F)$, the \cindex{Forbenius Inner Product} is defined as:
    \begin{equation}
        \innerproduct{A}{B} = \trace{B^* A}
    \end{equation}
\end{definition}

\begin{theorem}
    For square matrix $A_{n \times n}$, we have 
    \begin{equation}
        \innerproduct{A}{A} = \sum_{i=1}^n \sum_{j=1}^n \absolutevalue{A_{ij}}^2 \geq 0
    \end{equation}
\end{theorem}


\begin{definition}\label{hinnerproductspace}
	The continuous complex-valued function on interval $[0, 2\pi]$ is a inner product space $H$:
	\begin{equation}
		\innerproduct{f}{g} = \frac{1}{2\pi} \int_{0}^{2\pi} f(t) \overline{g(t)} dt
	\end{equation}
\end{definition}


\begin{definition}
	the \cindex{norm} or \cindex{length} of $x$ is:
	\begin{equation}
	    \norm{x} = \sqrt{\innerproduct{x}{x}}
	\end{equation}
\end{definition}

\begin{theorem}
	the property of norm:
	\begin{itemize}
		\item $\norm{cx} = \absolutevalue{c} \cdot \norm{x}$
		\item $\norm{x} = 0 \iff x = 0$
		\item \cindex{Cauchy-Schwarz Inequality} $\absolutevalue{\innerproduct{x}{y}} \leq \norm{x} \cdot \norm{y}$
		\item \cindex{Triangle Inequality} $\norm{x+y} \leq \norm{x} + \norm{y}$
	\end{itemize}
\end{theorem}

\begin{theorem}\label{zerotforalltx}
    If $\forall x \in C$,$\innerproduct{T(x)}{x} = 0$. Then $T = 0$.\footnote{For it to work in all $V$, $T$ needs to be self-adjoint. See \thmref{zerotforalltxforselfadjoint} on page \pageref{zerotforalltxforselfadjoint}.}
\end{theorem}
\begin{proof}
    \begin{equation*}
        \begin{aligned}
            \innerproduct{T(x+y)}{x+y} &= \innerproduct{T(x)}{y} + \innerproduct{T(y)}{x}  &= 0 \\
            \innerproduct{T(x+iy)}{x+iy} &= \innerproduct{T(x)}{y} -\innerproduct{T(y)}{x} &= 0            
        \end{aligned}
    \end{equation*}
    So $\forall y \in V$, $T(x) = 0$. So $\forall x \in V$,$T(x) = 0$ and $T = 0$.
\end{proof}

\begin{theorem}
    \begin{equation}
        \norm{u+v}^2 + \norm{u-v}^2 = 2 \left(\norm{u}^2 + \norm{v}^2 \right)
    \end{equation}    
\end{theorem}




% orthogonal

\subsection{Orthogonal and Gram-Schmidt Process}

\begin{definition}
	$x$ and $y$ are \cindex{orthogonal} if $\innerproduct{x}{y} = 0$. A subset $S$ of $V$ is orthogonal if any two vectors in $S$ are orthogonal. A subset $S$ of $V$ is \cindex{orthonormal} if $S$ is orthogonal and consists entirely of unit vectors.
\end{definition}

\begin{definition}
    \begin{equation}
        \innerproduct{x}{y} = \norm{x} \cdot \norm{y} \text{cos}(\theta)
    \end{equation}    
\end{definition}


\begin{definition}
	A vector is \cindex{unit vector} if $\norm{x} = 1$. A \cindex{normalizing} to non-zero $x$ is $\dfrac{1}{\norm{x}} x$.
\end{definition}


\begin{theorem}
    Let $f_n (t) = e^{i nt}$ where $0 \leq t \leq 2 \pi$. All $f_i$ are orthogonal.
\end{theorem}
\begin{proof}
    \begin{equation}
        \begin{aligned}
            \innerproduct{f_m}{f_n} &= \frac{1}{2 \pi} \int_0^{2 \pi} e^{imt} \overline{e^{int}} \dif{t} \\
            &= \frac{1}{2\pi} \int_0^{2\pi} e^{i (m-n) t} \dif{t} \\
            &= \eval{\frac{1}{2\pi (m-n)} e^{i(m-n)t}}_0^{2\pi} \\
            &= 0
        \end{aligned}
    \end{equation}
\end{proof}

\begin{theorem}[\cindex{Pythagorean Theorem}]
    Suppose $u$ and $v$ are orthogonal in $V$, then
    \begin{equation}
        \norm{u + v}^2 = \norm{u}^2 + \norm{v}^2
    \end{equation}    
\end{theorem}

\begin{theorem}
    For a finite dimensional subspace $U$ of $V$, we have
    \begin{equation}
        V = U \oplus U^\bot
    \end{equation}        
\end{theorem}



\begin{definition}
	A \cindex{orthonormal basis} for $V$ is an ordered basis that is orthonormal.
\end{definition}

\begin{theorem}
	Let $S=\set{ v_1, v_2, \dots, v_k }$ be an orthogonal subset of $V$ consisting of non-zero vectors. If $y \in \vectorspan{S}$, then
	\begin{equation}
		y = \sum_{i=1}^k \frac{\innerproduct{y}{v_i}}{\norm{v_i}^2} v_i
	\end{equation}
	Define the projection of vector $a$ onto vector $u$ as $\projection{a}{u} = \dfrac{\innerproduct{a}{u}}{\norm{u}^2}$. So 
	\begin{equation}
	    y = \sum_{i=1}^k \left(\projection{y}{v_i}\right) v_i
	\end{equation}
	If $S$ is orthonormal, then
	\begin{equation}
		y = \sum_{i=1}^k \innerproduct{y}{v_i} v_i
	\end{equation}
\end{theorem}
\begin{proof}
	let $\displaystyle y = \sum_{i=1}^k a_i v_i$. we have
	\begin{equation*}
		\innerproduct{y}{v_i} = \innerproduct{\sum_{i=1}^k a_i v_i}{v_j} = \sum_{i=1}^k a_i \innerproduct{v_i}{v_j} = a_j \norm{v_j}^2
	\end{equation*}
	So $a_j = \dfrac{\innerproduct{y}{v_j}}{\norm{v_j}^2}$.
	
	
\end{proof}

\begin{theorem}
	An orthogonal subset of $V$ is linearly independent.
\end{theorem}

\begin{definition}[\cindex{Gram-Schmidt process}]
	Let $S=\set{w_1, w_2, \dots, w_n }$ be linearly independent subset of $V$. Define $S^\prime=\{v_1,v_2,\dots,v_n  \}$, where $v_1=w_1$ and 
	\begin{equation}
		v_k = w_k - \sum_{j=1}^{k-1} \frac{\innerproduct{w_k}{v_j}}{\norm{v_j}^2} v_j
	\end{equation}
	then $S^\prime$ is an orthogonal set of non-zero vectors that $\vectorspan{S^\prime} = \vectorspan{S}$. The process is that for the $k$-th basis $w_k$, first project it on top of the $k-1$ orthogonal vectors $\displaystyle \sum_{j=1}^{k-1} \dfrac{\innerproduct{w_k}{v_j}}{\norm{v_j}^2} v_j$, and calculate the reciprocal vector $\displaystyle w_k - \sum_{j=1}^{k-1} \dfrac{\innerproduct{w_k}{v_j}}{\norm{v_j}^2} v_j$.
	\qed
\end{definition}

\begin{theorem}[\cindex{QR Decomposition}]
    Let $A_{m \times n} = \rowvector{a_1, a_2, \dots, a_n}$ with $\rank{A} = n$, so $\set{a_i}$ is linearly independent. Use Gram-Schmidt process to form $n$ orthonomal basis:
    \begin{equation*}
        \begin{aligned}
            u_1 &= a_1 & \text{ , } e_1 = \frac{u_1}{\norm{u_1}} \\
            u_2 &= a_2 - \projection{a_2}{u_1} & \text{ , } e_2 = \frac{u_2}{\norm{u_2}} \\
            \dots \\
            u_n &= a_n - \sum_{j=1}^{n-1} \projection{a_n}{u_j} & \text{ , } e_n = \frac{u_n}{\norm{u_n}} 
        \end{aligned}
    \end{equation*}
    Then $\forall k$, $\displaystyle a_k = \sum_{j=1}^k \innerproduct{a_k}{e_k} e_k$. So
    \begin{equation}
        A = QR = [e_1, e_2, \dots, e_n] \times \begin{bmatrix}
            \innerproduct{a_1}{e_1} & \innerproduct{a_2}{e_1} & \innerproduct{a_3}{e_1} & \cdots & \innerproduct{a_n}{e_1}\\
            0 & \innerproduct{a_2}{e_2} & \innerproduct{a_3}{e_2} & \cdots & \innerproduct{a_n}{e_2}\\
            0 & 0 & \innerproduct{a_3}{e_3} & \cdots & \innerproduct{a_n}{e_3} \\            
            \vdots & \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & 0 & 0 & \innerproduct{a_n}{e_n}
        \end{bmatrix}
    \end{equation}
    
    The $Q$ is an orthonormal matrix. $R$ could be calculated by:
    \begin{equation}
        R = Q^\top Q R = Q^\top A
    \end{equation}
\end{theorem}


\begin{theorem}\label{vectorinorthonormalbasis}
	If $V$ has an orthonormal basis $\beta=\set{v_1,v_2,\dots,v_n}$, then $\forall x\in V$, 
	\begin{equation}
		x = \sum_{i=1}^n \innerproduct{x}{v_j} v_i
	\end{equation}
\end{theorem}

\begin{definition}
    Let $\beta$ be an orthonormal subset (not basis) of $V$. For $x \in V$, the \cindex{Fourier coefficients} of $x$ relative to $\beta$ are $\innerproduct{x}{y_i}$ for all $y_i \in \beta$.
\end{definition}


\begin{theorem}\label{matrixelementasinnerproductresult}
	Let $V$ with an orthonormal basis $\beta=\set{v_1,v_2,\dots,v_n}$. $T$ is a linear operator on $V$ and let $A= \coordinate{T}_\beta$. then $A_{ij}=\innerproduct{T(v_j)}{v_i}$.
\end{theorem}
\begin{proof}
	From \thmref{vectorinorthonormalbasis} we have
	\begin{equation*}
		T(v_j) = \sum_{i=1}^n \innerproduct{T(v_j)}{v_i} v_i
	\end{equation*}
\end{proof}

\begin{definition}
    Let $S$ be nonempty subset of $V$. The \cindex{orthogonal complement} of $S$ is $S^\bot$ that $\forall x \in S, \forall y \in S^\bot, \innerproduct{x}{y} = 0$.
\end{definition}

\begin{theorem}\label{orthogonalprojection}
    Let $W$ be a subspace of $V$. For $y \in V$, there is \emph{unique} $u \in W$ and $z \in W^\bot$ that $y = u + z$. $u$ is the \cindex{orthogonal projection} of $y$ on $W$. If $\set{v_1, v_2, \dots, v_k}$ is an orthonormal basis of $W$, then 
    \begin{equation}
        \begin{aligned}
            u &= \sum_{i=1}^k \innerproduct{y}{v_i} v_i \\
            z &= y -   \sum_{i=1}^k \innerproduct{y}{v_i} v_i          
        \end{aligned}
    \end{equation}
\end{theorem}


\begin{theorem}
    For $S=\set{v_1, v_2, \dots, v_k}$ be an orthogonal subset of $V$. For $\forall y \in V$, the orthogonal projection of $y$ on $S$ is $\displaystyle u = \sum_{i=1}^k \dfrac{\innerproduct{y}{v_i}}{\norm{v_i}^2} v_i$. If $S$ are orthonormal, $\displaystyle u = \sum_{i=1}^k \innerproduct{y}{v_i} v_i$. If $y$ is in span of $S$, then $y = u$.
\end{theorem}



\begin{theorem}
    Let $y$,$u$,$z$ as defined in \thmref{orthogonalprojection}. $u$ is the closest vector in $W$ to $y$ that is $\forall x \in W \left(\norm{y-x} \geq \norm{y - u} \right)$.
\end{theorem}
\begin{proof}
    \begin{equation*}
        \norm{y - x}^2 = \norm{u + z - x}^2 = \norm{(u - x) + z}^2 = \norm{u - x}^2 + \norm{z}^2 \geq \norm{z}^2 = \norm{y - u}^2
    \end{equation*}
\end{proof}







% Adjoint of Linear Operator
\subsection{Adjoint of Linear Operator}

\begin{theorem}[\cindex{Riesz Representation Theorem}]\label{uniquelinearoperatortof}
    Let $g: V \rightarrow F$ be a linear transformation. Then there exist a unique $y \in V$ that $\forall x \in V$, $g(x) = \innerproduct{x}{y}$. The $y$ is 
    \begin{equation}
        y = \sum_{i=1}^n \overline{g(v_i)} v_i
    \end{equation}
    
    So every vector in the dual space\footnote{Defined in \thmref{dualspacedefinition} on page \pageref{dualspacedefinition}.} can be represented by an inner product.
\end{theorem}
\begin{proof}
    Define $h(x) = \innerproduct{x}{y}$ with $y$ defined above. So
    \begin{equation*}
        h(v_j) = \innerproduct{v_j}{y} = \innerproduct{v_j}{\sum_{i=1}^n \overline{g(v_i)} v_i} = \sum_{i=1}^n \innerproduct{v_j}{\overline{g(v_i)} v_i} = \sum_{i=1}^n g(v_i) \innerproduct{v_j}{ v_i} = g(v_j)
    \end{equation*}
\end{proof}



\begin{theorem}
    Let $T$ be a linear operator on $V$. Then there existing a unique linear operator $T^* : V \rightarrow V$ that $\innerproduct{T(x)}{y}=\innerproduct{x}{T^*(y)}$ for all $x,y \in V$. $T^*$ is called the \cindex{adjoint} of $T$.
\end{theorem}
\begin{proof}
    For each $y$, $\innerproduct{T(x)}{y}$ is a linear operator from $V$ to $F$, so by \thmref{uniquelinearoperatortof}, $\exists y'$ that $\innerproduct{T(x)}{y} = \innerproduct{x}{y'}$. Define $T^*$ as $T^*(y) = y'$.
\end{proof}

\begin{theorem}
    \begin{equation}
        \begin{aligned}
            \innerproduct{T(x)}{y} &= \innerproduct{x}{T^*(y)} \\
            \innerproduct{x}{T(y)} &= \innerproduct{T^*(x)}{y}
        \end{aligned}        
    \end{equation}
    So $^*$ is added to $T$ when change the location of $T$.
\end{theorem}
\begin{proof}
    \begin{equation*}
        \innerproduct{x}{T(y)} = \overline{\innerproduct{T(y)}{x}} = \overline{\innerproduct{y}{T^*(x)}} = \innerproduct{T^*(x)}{y}
    \end{equation*}
\end{proof}

\begin{theorem}
    Let $\beta$ be a orthonormal basis for $V$. If $T$ is a linear operation on $V$ then
    \begin{equation}
        [T^*]_\beta = \left([T]_\beta \right)^*
    \end{equation}
    Let $A$ be an $n \times n$ matrix. Then
    \begin{equation}
        L_{A^*} = \left(L_A \right)^*
    \end{equation}
\end{theorem}
\begin{proof}
    Let $A=[T]_\beta$, $B=[T^*]_\beta$, and $\beta=\{v_1, v_2, \dots, v_n \}$. Then according to \thmref{matrixelementasinnerproductresult}:
    \begin{equation*}
        B_{ij} = \innerproduct{T^*(v_j)}{v_i} = \overline{\innerproduct{v_i}{T^*(v_j)}} = \overline{\innerproduct{T(v_i)}{v_j}} = \overline{A_{ji}} = (A^*)_{ij}
    \end{equation*}
\end{proof}

\begin{theorem}
    Let $T$ and $U$ be linear operator on $V$, then
    \begin{enumerate}
        \item $(aT+bU)^* = \overline{a} T^* + \overline{b} U^*$
        \item $(UT)^* = T^* U^*$
        \item $T^{**} = T$
    \end{enumerate}    
\end{theorem}


\begin{definition}
    Let $T : V \rightarrow W$ be a linear transformation where $V$ and $W$ are finite dimensional inner product space with inner product $\innerproduct{\cdot{}}{\cdot{}}_V$ and $\innerproduct{\cdot{}}{\cdot{}}_W$. A function $T^* : W \rightarrow V$ is called \cindex{adjoint} of $T$ if $\innerproduct{T(x)}{y}_W = \innerproduct{x}{T^*(y)}_V$.
\end{definition}

\begin{theorem}
    Let $T^*$ be an adjoint of $T: V \rightarrow W$. If $\beta$ and $\gamma$ are orthonormal basis for $V$ and $W$, then
    \begin{equation}
        [T^*]_\beta^\alpha = \left([T]_\beta^\alpha\right)^*
    \end{equation}
\end{theorem}

\begin{theorem}
    Let $T^*$ be an adjoint of $T: V \rightarrow W$, we have:
    \begin{equation}
        \innerproduct{T^*(x)}{y}_V = \innerproduct{x}{T(y)}_W
    \end{equation}
\end{theorem}



\begin{theorem}\label{nullandreciprocaladjoint}
    If $V$ is finite dimentional, let $T$ be a linear operator on $V$, then
    \begin{equation*}
        \begin{aligned}
            \rangespace{T^*}^\bot &= \nullspace{T}\\
            \rangespace{T^*} &= \nullspace{T}^\bot \\
            \rangespace{T}^\bot &= \nullspace{T^*}\\
            \rangespace{T} &= \nullspace{T^*}^\bot
        \end{aligned}
    \end{equation*}
    So $\rangespace{T^*} \bot \nullspace{T}$.
\end{theorem}
\begin{proof}
    If $m \in R(T^*)^\bot$, $\forall x \in V$, $0 = \innerproduct{m}{T^*x} = \innerproduct{T(m)}{x}$, so $m \in N(T)$.
\end{proof}



% Example in statistics
\subsection{Examples in Statistics}\label{consistentandinconsistentequation}

The following two examples show that for linear equation $Ax - y = 0$, 
\begin{enumerate}
    \item if it is consistent, that is there is solution, we want to find the solution with minimal norm.
    \item If it is inconsistent, that is no solution, we want a result that has the least norm.
\end{enumerate}

The same topic is discussed in pseudo inverse.


% Least Square Approximation
\subsubsection{Least Square Approximation}

\begin{definition}
    The \cindex{Least Square Approximation} is a problem that for $A = \begin{bmatrix}
        t_1 & 1 \\
        t_2 & 1 \\
        \vdots & \vdots \\
        t_m & 1
    \end{bmatrix}$,  $y = \begin{bmatrix}
        y_1\\
        y_2\\
        \vdots\\
        y_m
    \end{bmatrix}$, find $x_0 = \begin{bmatrix}
        c \\
        d
    \end{bmatrix}$ that minimize $\norm{Ax-y}$.
\end{definition}

\begin{definition}
    For $x,y \in F^n$, define $\innerproduct{x}{y}_n = y^* \times x$.
\end{definition}


\begin{theorem}
    Let $A \in M_{m \times n} (F)$, $x \in F^n$, $y\in F^m$, then
    \begin{equation}
        \innerproduct{Ax}{y}_m = \innerproduct{x}{A^* y}_n
    \end{equation}
\end{theorem}
\begin{proof}
    $\innerproduct{Ax}{y}_m = y^* \times (Ax) = (y^* \times A) x = (A^* y)^* x = \innerproduct{x}{A^* y}_n$
\end{proof}

\begin{theorem}
    Let $A \in M_{m\times n} (F)$. Then\footnote{See \thmref{rankofadjoint} for another proof.}
    \begin{equation}
        \rank{A^*A} = \rank{A}
    \end{equation}
    So if $\rank{A} = n$, $A^*A$ is invertible.
\end{theorem}
\begin{proof}
    For equation $A^*Ax = 0$ and $Ax = 0$. $Ax=0$ implies that $A^*Ax =0$. Then assume $A^*Ax = 0$, then
    \begin{equation*}
        0 = \innerproduct{0}{x}_n = \innerproduct{A^*Ax}{x}_n = \innerproduct{Ax}{A^{**}x}_m = \innerproduct{Ax}{Ax}_m
    \end{equation*}
\end{proof}


\begin{theorem}
    Let $A \in M_{m\times n} (F)$, $y \in F^m$. Then there exists $x_0 \in F^n$ that $(A^*A) x_0 = A^* y$ and $\forall x \in F^n$, $ \norm{Ax_0 - y} \leq \norm{Ax-y}$. If $\rank{A} = n$, then $x_0 = (A^*A)^{-1} A^* y$.
\end{theorem}
\begin{proof}
    Define $W=\rangespace{L_A}$. There exists a $x_0$ that is closest to $y$ that $Ax_0 - y \in W^\bot$, so $\innerproduct{Ax}{Ax_0 - y}_m = 0$. So $\innerproduct{x}{A^*(Ax_0 - y)}_n = 0$, so $A^*(Ax_0 - y) = 0$ and $(A^*A) x_0 = A^* y$. 
\end{proof}




% Minimal Solution to Linear Equations
\subsubsection{Minimal Solution to Linear Equations}

\begin{definition}
    A solution $s$ is \cindex{minimal solution} of $Ax=b$ if $\norm{s} \leq \norm{u}$ for any solution $u$.
\end{definition}



\begin{theorem}
    Let $A \in M_{m\times n} (F)$, $y \in F^m$. Suppose $Ax=y$ is consistent. Then there exists unique minimal solution $s \in R(L_{A^*})$ of $Ax=y$. And $s$ is the only solution in $R(L_{A^*})$. If $u$ is a solution to $(AA^*) u = y$, then $s = A^* u$.
\end{theorem}
\begin{proof}
    By \thmref{nullandreciprocaladjoint} define $W = R(L_{A^*})$ and $W^\bot = N(L_A)$. $\forall x$ that $Ax = y$, we have $s \in W$ and $t \in W^\bot$ that $x=s+t$. So $y = Ax = A(s + t) = As + At = As$. So $s$ is a solution to $Ax=y$. From \thmref{equationfromoneandnullspace}, all solution to $Ax=y$ has the form $x' = s + t'$ where $t' \in W^\bot$. And $\norm{x'}^2 = \norm{s + t'}^2 = \norm{s}^2 + \norm{t'}^2 \geq \norm{s}^2$.
\end{proof}





























































































































































