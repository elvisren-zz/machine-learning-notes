\section{Inner Product Space}


\subsection{Inner Product and Norm}

\begin{definition}
	An \cindex{inner product} on $V$ is a function $V \rightarrow V \rightarrow F$ ($F$ is either $C$ or $R$) that $\forall x,y,z \in V$ and $\forall c \in F$ that:
	\begin{enumerate}
		\item $\innerproduct{x+z}{y} = \innerproduct{x}{y} + \innerproduct{z}{y}$ \label{firstproductdefinition}
		\item $\innerproduct{cx}{y} = c \innerproduct{x}{y}$ \label{secondproductdefinition}
		\item $\overline{\innerproduct{x}{y}} = \innerproduct{y}{x}$
		\item $\innerproduct{x}{x} \geq 0$
	\end{enumerate}
	Item (\ref{firstproductdefinition}) and (\ref{secondproductdefinition}) means the inner product is \emph{linear in first component}.
	Please be noted that the result of inner product could be a complex value.
	\qed
\end{definition}


\begin{theorem}
	properties of inner product:
	\begin{enumerate}
		\item $\innerproduct{x}{y+z} = \innerproduct{x}{y} + \innerproduct{x}{z}$ \label{firstproductproperty}
		\item $\innerproduct{x}{cy} = \overline{c} \innerproduct{x}{y} $ \label{secondproductproperty}
		\item $\innerproduct{x}{x} > 0$ unless $x=0$
		\item If $\innerproduct{x}{y} = \innerproduct{x}{z}$ for all $x \in V$, then $y=z$.
	\end{enumerate}
	Item (\ref{firstproductproperty}) and (\ref{secondproductproperty}) means the inner product is \emph{conjugate linear in second component}.
\end{theorem}


\begin{definition}
	the \cindex{standard inner product} on $F^n$ for $x=(a_1,a_2,\dots,a_n)$ and $y=(b_1,b_2,\dots,b_n)$ is:
	\begin{equation}
		\innerproduct{x}{y} = \sum_{i=1}^n a_i \overline{b_i}		
	\end{equation}
	when $F=R$, it is usually called \cindex{dot product} and denoted as $x \cdot y$.
\end{definition}

\begin{definition}
	For $A \in M_{m \times n}(F)$, the \cindex{conjugate transpose} or \cindex{adjoint} of $A$ is $A^* \in M_{n \times m}(F)$ that $(A^*)_{ij} = \overline{A_{ji}}$. If $A$ is complex, $A^* = \overline{A^\top}$ .If $A$ is real, $A^*$ is $A^\top$.
\end{definition}

\begin{definition}[Forbenius Inner Product]
    Let $V=M_{n \times n} (F)$, the \cindex{Forbenius Inner Product} is defined as:
    \begin{equation}
        \innerproduct{A}{B} = \text{tr}(B^* A)
    \end{equation}
\end{definition}

\begin{definition}\label{hinnerproductspace}
	The continuous complex-valued function on interval $[0, 2\pi]$ is a inner product space $H$:
	\begin{equation}
		\innerproduct{f}{g} = \frac{1}{2\pi} \int_{0}^{2\pi} f(t) \overline{g(t)} dt
	\end{equation}
\end{definition}


\begin{definition}
	the \cindex{norm} or \cindex{length} of $x$ is:
	\begin{equation}
	    \norm{x} = \sqrt{\innerproduct{x}{x}}
	\end{equation}
\end{definition}

\begin{theorem}
	the property of norm:
	\begin{itemize}
		\item $\norm{cx} = \absolutevalue{c} \times \norm{x}$
		\item $\norm{x} = 0 \iff x = 0$
		\item \cindex{Cauchy-Schwarz Inequality} $\absolutevalue{\innerproduct{x}{y}} \leq \norm{x} \cdot \norm{y}$
		\item \cindex{Triangle Inequality} $\norm{x+y} \leq \norm{x} + \norm{y}$
	\end{itemize}
\end{theorem}

\begin{theorem}\label{zerotforalltx}
    If $\forall x \in \mathcal{C}$,$\innerproduct{T(x)}{x} = 0$. Then $T = 0$.\footnote{For it to work in all $V$, $T$ needs to be self-adjoint. See Theorem (\ref{zerotforalltxforselfadjoint}) on page \pageref{zerotforalltxforselfadjoint}}
\end{theorem}
\begin{proof}
    \begin{equation*}
        \begin{aligned}
            \innerproduct{T(x+y)}{x+y} &= \innerproduct{T(x)}{y} + \innerproduct{T(y)}{x}  &= 0 \\
            \innerproduct{T(x+iy)}{x+iy} &= \innerproduct{T(x)}{y} -\innerproduct{T(y)}{x} &= 0            
        \end{aligned}
    \end{equation*}
    So $\forall y \in V$, $T(x) = 0$. So $\forall x \in V$,$T(x) = 0$ and $T = 0$.
\end{proof}





% orthogonal

\subsection{Orthogonal and Gram-Schmidt Process}

\begin{definition}
	$x$ and $y$ are \cindex{orthogonal} if $\innerproduct{x}{y} = 0$. A subset $S$ of $V$ is orthogonal if any two vectors in $S$ are orthogonal. A subset $S$ of $V$ is \cindex{orthonormal} if $S$ is orthogonal and consists entirely of unit vectors.
\end{definition}

\begin{definition}
    \begin{equation}
        \innerproduct{x}{y} = \norm{x} \cdot \norm{y} \text{cos}(\theta)
    \end{equation}    
\end{definition}


\begin{definition}
	A vector is \cindex{unit vector} if $\norm{x} = 1$. A \cindex{normalizing} to non-zero $x$ is $\dfrac{1}{\norm{x}} x$.
\end{definition}


\begin{theorem}
    Let $f_n (t) = e^{i nt}$ where $0 \leq t \leq 2 \pi$. All $f_i$ are orthogonal.
\end{theorem}
\begin{proof}
    \begin{equation}
        \begin{aligned}
            \innerproduct{f_m}{f_n} &= \frac{1}{2 \pi} \int_0^{2 \pi} e^{imt} \overline{e^{int}} \dif{t} \\
            &= \frac{1}{2\pi} \int_0^{2\pi} e^{i (m-n) t} \dif{t} \\
            &= \left. \frac{1}{2\pi (m-n)} e^{i(m-n)t} \right|_0^{2\pi} \\
            &= 0
        \end{aligned}
    \end{equation}
\end{proof}



\begin{definition}
	A \cindex{orthonormal basis} for $V$ is an ordered basis that is orthonormal.
\end{definition}

\begin{theorem}
	let $S=\{ v_1, v_2, \dots, v_k \}$ be an orthogonal subset of $V$ consisting of non-zero vectors. If $y \in \text{span}(S)$, then
	\begin{equation}
		y = \sum_{i=1}^k \frac{\innerproduct{y}{v_i}}{\norm{v_i}^2} v_i
	\end{equation}
	If $S$ is orthonormal, then
	\begin{equation}
		y = \sum_{i=1}^k \innerproduct{y}{v_i} v_i
	\end{equation}
\end{theorem}
\begin{proof}
	let $\displaystyle y = \sum_{i=1}^k a_i v_i$. we have
	\begin{equation*}
		\innerproduct{y}{v_i} = \innerproduct{\sum_{i=1}^k a_i v_i}{v_j} = \sum_{i=1}^k a_i \innerproduct{v_i}{v_j} = a_j \norm{v_j}^2
	\end{equation*}
	So $\displaystyle a_j = \frac{\innerproduct{y}{v_j}}{\norm{v_j}^2}$.
	
	
\end{proof}

\begin{theorem}
	an orthogonal subset of $V$ is linearly independent.
\end{theorem}

\begin{definition}[\cindex{Gram-Schmidt process}]
	Let $S=\{w_1, w_2, \dots, w_n \}$ be linearly independent subset of $V$. Define $S^\prime=\{v_1,v_2,\dots,v_n  \}$, where $v_1=w_1$ and 
	\begin{equation}
		v_k = w_k - \sum_{j=1}^{k-1} \frac{\innerproduct{w_k}{v_j}}{\norm{v_j}^2} v_j
	\end{equation}
	then $S^\prime$ is an orthogonal set of non-zero vectors that $\text{span}(S^\prime) = \text{span}(S)$. The process is that for the $k$th basis $w_k$, first project it on top of the $k-1$ orthogonal vectors $\displaystyle \sum_{j=1}^{k-1} \frac{\innerproduct{w_k}{v_j}}{\norm{v_j}^2} v_j$, and calculate the reciprocal vector $\displaystyle w_k - \sum_{j=1}^{k-1} \frac{\innerproduct{w_k}{v_j}}{\norm{v_j}^2} v_j$.
	\qed
\end{definition}

\begin{theorem}\label{vectorinorthonormalbasis}
	If $V$ has an orthonormal basis $\beta=\{v_1,v_2,\dots,v_n\}$, then $\forall x\in V$, 
	\begin{equation}
		x = \sum_{i=1}^n \innerproduct{x}{v_j} v_i
	\end{equation}
\end{theorem}

\begin{definition}
    Let $\beta$ be an orthonormal subset of $V$. For $x \in V$, the \cindex{Fourier coefficients} of $x$ relative to $\beta$ are $\innerproduct{x}{y_i}$ for all $y_i \in \beta$.
\end{definition}


\begin{theorem}
	Let $V$ with an orthonormal basis $\beta=\{v_1,v_2,\dots,v_n\}$. $T$ is a linear operator on $V$ and let $A=[T]_\beta$. then $A_{ij}=\innerproduct{T(v_j)}{v_i}$.
\end{theorem}
\begin{proof}
	From theorem (\ref{vectorinorthonormalbasis}) we have
	\begin{equation*}
		T(v_j) = \sum_{i=1}^n \innerproduct{T(v_j)}{v_i} v_i
	\end{equation*}
\end{proof}

\begin{definition}
    Let $S$ be nonempty subset of $V$. The \cindex{orthogonal complement} of $S$ is $S^\bot$ that $\forall x \in S, \forall y \in S^\bot, \innerproduct{x}{y} = 0$.
\end{definition}

\begin{theorem}\label{orthogonalprojection}
    Let $W$ be a subspace of $V$. For $y \in V$, there is unique $u \in W$ and $z \in W^\bot$ that $y = u + z$. $u$ is the \cindex{orthogonal projection} of $y$ on $W$. If $\{v_1, v_2, \dots, v_k \}$ is an orthonormal basis of $W$, then 
    \begin{equation}
        \begin{aligned}
            u &= \sum_{i=1}^k \innerproduct{y}{v_i} v_i \\
            z &= y -   \sum_{i=1}^k \innerproduct{y}{v_i} v_i          
        \end{aligned}
    \end{equation}
\end{theorem}


\begin{theorem}
    For $S=\{v_1, v_2, \dots, v_k \}$ be an orthogonal subset of $V$. For $\forall y \in V$, the orthogonal projection of $y$ on $S$ is $\displaystyle u = \sum_{i=1}^k \frac{\innerproduct{y}{v_i}}{\norm{v_i}^2} v_i$. If $S$ are orthonormal, $\displaystyle u = \sum_{i=1}^k \innerproduct{y}{v_i} v_i$. If $y$ is in span of $S$, then $y = u$.
\end{theorem}



\begin{theorem}
    Let $y$,$u$,$z$ as defined in Theorem (\ref{orthogonalprojection}). $u$ is the closest vector in $W$ to $y$ that is $\forall x \in W$, $\norm{y-x} \geq \norm{y - u}$.
\end{theorem}
\begin{proof}
    \begin{equation*}
        \begin{aligned}
            \norm{y - x}^2 &= \norm{u + z - x}^2 \\
            &= \norm{(u - x) + z}^2 \\
            &= \norm{u - x}^2 + \norm{z}^2 \\
            &\geq \norm{z}^2 = \norm{y - u}^2
        \end{aligned}
    \end{equation*}
\end{proof}





% Adjoint of Linear Operator
\subsection{Adjoint of Linear Operator}

\begin{theorem}\label{uniquelinearoperatortof}
    Let $g: V \rightarrow F$ be a linear transformation. Then there exist a unique $y \in V$ that $\forall x \in V$, $g(x) = \innerproduct{x}{y}$. The $y$ is 
    \begin{equation}
        y = \sum_{i=1}^n \overline{g(v_i)} v_i
    \end{equation}
\end{theorem}

\begin{theorem}
    Let $T$ be a linear operator on $V$. Then there existing a unique linear operator $T^* : V \rightarrow V$ that $\innerproduct{T(x)}{y}=\innerproduct{x}{T^*(y)}$ for all $x,y \in V$. $T^*$ is called the \cindex{adjoint} of $T$.
\end{theorem}
\begin{proof}
    For each $y$, $\innerproduct{T(x)}{y}$ is a linear operator from $V$ to $F$, so by Theorem (\ref{uniquelinearoperatortof}) $\exists y'$ that $\innerproduct{T(x)}{y} = \innerproduct{x}{y'}$.
\end{proof}

\begin{theorem}
    \begin{equation}
        \begin{aligned}
            \innerproduct{T(x)}{y} &= \innerproduct{x}{T^*(y)} \\
            \innerproduct{x}{T(y)} &= \innerproduct{T^*(x)}{y}
        \end{aligned}        
    \end{equation}
    So a $^*$ is added to $T$ when change the location of $T$.
\end{theorem}
\begin{proof}
    \begin{equation*}
        \innerproduct{x}{T(y)} = \overline{\innerproduct{T(y)}{x}} = \overline{\innerproduct{y}{T^*(x)}} = \innerproduct{T^*(x)}{y}
    \end{equation*}
\end{proof}

\begin{theorem}
    Let $\beta$ be a orthonormal basis for $V$. If $T$ is a linear operation on $V$ then
    \begin{equation}
        [T^*]_\beta = ([T]_\beta)^*
    \end{equation}
    Let $A$ be an $n \times n$ matrix. Then
    \begin{equation}
        L_{A^*} = (L_A)^*
    \end{equation}
\end{theorem}
\begin{proof}
    Let $A=[T]_\beta$, $B=[T^*]_\beta$, and $\beta=\{v_1, v_2, \dots, v_n \}$. Then
    \begin{equation*}
        B_{ij} = \innerproduct{T^*(v_j)}{v_i} = \overline{\innerproduct{v_i}{T^*(v_j)}} = \overline{\innerproduct{T(v_i)}{v_j}} = \overline{A_{ji}} = (A^*)_{ij}
    \end{equation*}
\end{proof}

\begin{theorem}
    Let $T$ and $U$ be linear operator on $V$, then
    \begin{enumerate}
        \item $(aT+bU)^* = \overline{a}T^* + \overline{b}U^*$
        \item $(UT)^* = T^* U^*$
        \item $T^{**} = T$
    \end{enumerate}    
\end{theorem}


\begin{definition}
    Let $T : V \rightarrow W$ be a linear transformation where $V$ and $W$ are finite dimensional inner product space with inner product $\innerproduct{\cdot{}}{\cdot{}}_V$ and $\innerproduct{\cdot{}}{\cdot{}}_W$. A function $T^* : W \rightarrow V$ is called \cindex{adjoint} of $T$ if $\innerproduct{T(x)}{y}_W = \innerproduct{x}{T^*(y)}_V$.
\end{definition}

\begin{theorem}
    Let $T^*$ be an adjoint of $T: V \rightarrow W$. If $\beta$ and $\gamma$ are orthonormal basis for $V$ and $W$, then
    \begin{equation}
        [T^*]_\beta^\alpha = ([T]_\beta^\alpha)^*
    \end{equation}
\end{theorem}

\begin{theorem}
    Let $T^*$ be an adjoint of $T: V \rightarrow W$, we have:
    \begin{equation}
        \innerproduct{T^*(x)}{y}_V = \innerproduct{x}{T(y)}_W
    \end{equation}
\end{theorem}



\begin{theorem}\label{nullandreciprocaladjoint}
    If $V$ is finite dimentional, let $T$ be a linear operator on $V$, then
    \begin{equation*}
        \begin{aligned}
            \rangespace{T^*}^\bot &= \nullspace{T}\\
            \rangespace{T^*} &= \nullspace{T}^\bot
        \end{aligned}
    \end{equation*}
    So $\rangespace{T^*} \bot \nullspace{T}$.
\end{theorem}
\begin{proof}
    If $m \in R(T^*)^\bot$, $\forall x \in V$, $0 = \innerproduct{m}{T^*x} = \innerproduct{T(m)}{x}$, so $m \in N(T)$.
\end{proof}



% Example in statistics
\subsection{Examples in Statistics}

The following two examples show that for linear equation $Ax - y = 0$, 
\begin{enumerate}
    \item if it is consistent, that is there is solution, we want to find the solution with minimal norm.
    \item If it is inconsistent, that is no solution, we want a result that give the least error norm.
\end{enumerate}


% Least Square Approximation
\subsubsection{Least Square Approximation}


\begin{definition}
    The \cindex{Least Square Approximation} is a problem that for $A = \begin{pmatrix}
        t_1 & 1 \\
        t_2 & 1 \\
        \vdots & \vdots \\
        t_m & 1
    \end{pmatrix}$,  $y = \begin{pmatrix}
        y_1\\
        y_2\\
        \vdots\\
        y_m
    \end{pmatrix}$, find $x_0 = \begin{pmatrix}
        c \\
        d
    \end{pmatrix}$ that minimize $\norm{Ax-y}$.
\end{definition}

\begin{definition}
    For $x,y \in F^n$, define $\innerproduct{x}{y}_n = y^* \times x$.
\end{definition}


\begin{theorem}
    Let $A \in M_{m \times n} (F)$, $x \in F^n$, $y\in F^m$, then
    \begin{equation}
        \innerproduct{Ax}{y}_m = \innerproduct{x}{A^* y}_n
    \end{equation}
\end{theorem}
\begin{proof}
    $\innerproduct{Ax}{y}_m = y^* \times (Ax) = (y^* \times A) x = (A^* y)^* x = \innerproduct{x}{A^* y}_n$
\end{proof}

\begin{theorem}
    Let $A \in M_{m\times n} (F)$. Then
    \begin{equation}
        \rank{A^*A} = \rank{A}
    \end{equation}
    So if $\rank{A} = n$, $A^*A$ is invertible.
\end{theorem}
\begin{proof}
    For equation $A^*Ax = 0$ and $Ax = 0$. $Ax=0$ implies that $A^*Ax =0$. Then assume $A^*Ax = 0$, then
    \begin{equation*}
        0 = \innerproduct{0}{x}_n = \innerproduct{A^*Ax}{x}_n = \innerproduct{Ax}{A^{**}x}_m = \innerproduct{Ax}{Ax}_m
    \end{equation*}
\end{proof}


\begin{theorem}
    Let $A \in M_{m\times n} (F)$, $y \in F^m$. Then there exists $x_0 \in F^n$ that $(A^*A) x_0 = A^* y$ and $\forall x \in F^n$, $ \norm{Ax_0 - y} \leq \norm{Ax-y}$. If $\rank{A} = n$, then $x_0 = (A^*A)^{-1} A^* y$.
\end{theorem}
\begin{proof}
    Define $W=\rangespace{L_A}$. There exists a $x_0$ that is closest to $y$ that $Ax_0 - y \in W^\bot$, so $\innerproduct{Ax}{Ax_0 - y}_m = 0$. So $\innerproduct{x}{A^*(Ax_0 - y)}_n = 0$, so $A^*(Ax_0 - y) = 0$ and $(A^*A) x_0 = A^* y$. 
\end{proof}




% Minimal Solution to Linear Equations
\subsubsection{Minimal Solution to Linear Equations}

\begin{definition}
    A solution $s$ is \cindex{minimal solution} of $Ax=b$ if $\norm{s} \leq \norm{u}$ for any solution $u$.
\end{definition}



\begin{theorem}
    Let $A \in M_{m\times n} (F)$, $y \in F^m$. Suppose $Ax=y$ is consistent. Then there exists unique minimal solution $s \in R(L_{A^*})$ of $Ax=y$. And $s$ is the only solution in $R(L_{A^*})$. If $u$ is a solution to $(AA^*) u = y$, then $s = A^* u$.
\end{theorem}
\begin{proof}
    By Theorem (\ref{nullandreciprocaladjoint}) define $W = R(L_{A^*})$ and $W^\bot = N(L_A)$. $\forall x$ that $Ax = y$, we have $s \in W$ and $t \in W^\bot$ that $x=s+t$. So $y = Ax = A(s + t) = As + At = As$. So $s$ is a solution to $Ax=y$. From Theorem (\ref{equationfromoneandnullspace}), all solution to $Ax=y$ has the form $x' = s + t'$ where $t' \in W^\bot$. And $\norm{x'}^2 = \norm{s + t'}^2 = \norm{s}^2 + \norm{t'}^2 \geq \norm{s}^2$.
\end{proof}







% Normal
\subsection{Normal}

\begin{theorem}\label{eigenvectorforadjointoperator}
    If $T$ has eigenvector, then $T^*$ has eigenvector.    
\end{theorem}
\begin{proof}
    $0 = \innerproduct{0}{x} = \innerproduct{(T - \lambda I)(v)}{x} = \innerproduct{v}{(T - \lambda I)^* (x)} = \innerproduct{v}{(T^* - \overline{\lambda} I)(x)}$. Since $v \neq 0$ is reciprocal to the range of $T^* - \overline{\lambda} I$, $v \notin \rangespace{T^* - \overline{\lambda} I}$, so $\nullspace{T^* - \overline{\lambda} I} \neq \{ 0 \}$.
\end{proof}

\begin{theorem}[\cindex{Schur}]\label{schurincomplexfield}
    Suppose the characteristic polynomial of $T$ splits. Then there exists an orthonormal basis $\beta$ for $V$ that the $[T]_\beta$ is upper trianglar. Note:
    \begin{enumerate}
        \item $\beta$ does \emph{not} need to be eigenvectors of $T$.
        \item It works in $\mathcal{R}$ as long as $T$ splits.
    \end{enumerate} 
\end{theorem}
\begin{proof}
    Use induction. Since $T$ splits, it has a eigenvector. By Theorem (\ref{eigenvectorforadjointoperator}) $T^*$ has eigenvector, and make it a unit eigenvector $z$. Let $W = \text{span}\{z\}$. Then prove $W^\bot$ is $T$-invariant: for $\forall y \in W^\bot$ and $x = cz \in W$:
    \begin{equation*}
        \begin{aligned}
            \innerproduct{T(y)}{x} = \innerproduct{T(y)}{cz} = \innerproduct{y}{T^*(cz)} = \innerproduct{y}{cT^*(z)} = \innerproduct{y}{c \lambda z} = \overline{c\lambda} \innerproduct{y}{z} = 0
        \end{aligned}
    \end{equation*}
    According to induction, $\dimension{W^\bot} = n - 1$ and there exists an orthonormal basis $\gamma$ that $[T_{W^\bot}]_\gamma$ is upper triangular. Take $\gamma \cup \{z \}$.
\end{proof}

\begin{theorem}
    If $[T]_\beta$ is a diagonal matrix, $[T^*]_\beta = ([T]_\beta)^*$ is also a diagonal matrix.
\end{theorem}



\begin{definition}
    $T$ is \cindex{normal} if $T T^* = T^* T$. A square matrix $A$ is \cindex{normal} if $AA^* = A^* A$.
\end{definition}

\begin{theorem}
    $T$ is normal if and only of $[T]_\beta$ is normal under orthonormal basis $\beta$.
\end{theorem}

\begin{theorem}\label{propertyofnormaloperator}
    Properties of normal operator $T$ on $V$:
    \begin{enumerate}
        \item $\forall x \in V$, $\norm{T(x)} = \norm{T^*(x)}$
        \item $\forall c \in F$, $T - cI$ is normal.
        \item If $x$ is a eigenvector of eigenvalue $\lambda$ for $T$, $T^*(x) = \overline{\lambda} x$, so $x$ is also an eigenvector of eigenvalue $\overline{\lambda}$ for $T^*$.
        \item If $x_1$ and $x_2$ are for eigenvalues $\lambda_1$ and $\lambda_2$, $\innerproduct{x_1}{x_2} = 0$
    \end{enumerate}    
\end{theorem}
\begin{proof}
    \begin{equation*}
        \norm{T(x)}^2 = \innerproduct{T(x)}{T(x)} = \innerproduct{T^* T (x)}{x} = \innerproduct{TT^*(x)}{x} = \innerproduct{T^*(x)}{T^*(x)} = \norm{T^*(x)^2}
    \end{equation*}
    
    \begin{equation*}
        0 = \norm{(T - \lambda I)(x)} = \norm{(T - \lambda I)^*(x)} = \norm{(T^* - \overline{\lambda} I)(x) }
    \end{equation*}
    
    \begin{equation*}
        \lambda_1 \innerproduct{x_1}{x_2} = \innerproduct{\lambda x_1}{x_2} = \innerproduct{T(x_1)}{x_2} = \innerproduct{x_1}{T^*(x_2)} = \innerproduct{x_1}{\overline{\lambda_2} x_2} = \lambda_2 \innerproduct{x_1}{x_2}
    \end{equation*}
    So $(\lambda_1 - \lambda_2) \innerproduct{x_1}{x_2} = 0$. Since $\lambda_1 \neq \lambda_2$, $\innerproduct{x_1}{x_2} = 0$
\end{proof}


\begin{theorem}
    If $T$ is normal, $\nullspace{T} = \nullspace{T^*}$ and $\rangespace{T} = \rangespace{T^*}$. So being normal will refine Theorem (\ref{nullandreciprocaladjoint}).
\end{theorem}
\begin{proof}
    If $x \in \nullspace{T}$, $\norm{T(x)} = \norm{T^*} = 0$, so $T^*(x) = 0$ and $x \in \nullspace{T^*}$.
\end{proof}



\begin{theorem}
    In $\mathcal{C}$, let $V$ be finite dimensional inner product space. $T$ is normal if and only if there exists an orthonormal basis for $V$ consisting of eigenvectors of $T$.
\end{theorem}
\begin{proof}
    in $C$ the polynomial always splits. According to Theorem (\ref{schurincomplexfield}) there exists a orthonormal basis $\beta = \{v_1, v_2, \dots, v_n\}$ that $[T]_\beta = A$ is upper triangular. $v_1$ is an eigenvector because $T(v_1)=A_{1,1} v_1$. Assuming $v_1, v_2, \dots, v_{k-1}$ are eigenvector of $T$, we prove that $v_k$ is also an eigenvector of $T$. Because $A$ is upper triangular, 
    \begin{equation*}
        T(v_k) = A_{1,k} v_1 + A_{2,k} v_2 + \dots + A_{j,k} v_j + \dots + A_{k,k} v_k
    \end{equation*}
    Because $\forall j < k$, $A_{j,k} = \innerproduct{T(v_k}{v_j} = \innerproduct{v_k}{T^*(v_j)} = \innerproduct{v_k}{\overline{\lambda} v_j} = \lambda_j \innerproduct{v_k}{v_j} = 0$, we have $T(v_k) = A_{k,k} v_k$, so $v_k$ is an eigenvector of $T$.
    
    btw, it does not work in infinite dimensional complex inner product space.
\end{proof}






% self-adjoint
\subsection{Self-adjoint}

\begin{definition}
    $T$ is \cindex{self-adjoint} (\cindex{Hermitian}) if $T = T^*$, or $A = A^*$. For real matrix, it means $A$ is symmetric.
\end{definition}

\begin{theorem}
    Let $T$ be a linear operator on complex inner product space. Then $T$ is self-adjoint if and only if $\forall x \in V$,$\innerproduct{T(x)}{x} \in \mathcal{R}$.
\end{theorem}
\begin{proof}
    If $T$ is self-adjoint, $\overline{\innerproduct{T(x)}{x}} = \innerproduct{x}{T(x)} = \innerproduct{T^*(x)}{x} = \innerproduct{T(x)}{x}$. So $\innerproduct{T(x)}{x} \in \mathcal{R}$.
    
    If $\innerproduct{T(x)}{x} \in \mathcal{R}$, $\innerproduct{T(x)}{x} = \overline{\innerproduct{T(x)}{x}} = \innerproduct{x}{T(x)} = \innerproduct{T^*(x)}{x}$. So $\forall x \in V$, $\innerproduct{(T - T^*)(x)}{x} = 0$. According to Theorem (\ref{zerotforalltx}), $T - T^* = 0$.
\end{proof}


\begin{theorem}
    Let $T$ be a self-adjoint operator on finite dimensional inner product space $V$. Then:
    \begin{enumerate}
        \item every eigenvalue is real.
        \item If $V$ is a real inner product space, the characteristic polynomial for $T$ splits.
    \end{enumerate}
\end{theorem}
\begin{proof}
    Because $T$ is self-adjoint, $T$ is also normal. So according to Theorem (\ref{propertyofnormaloperator}) if $\lambda$ is an eigenvalue of $T$,  $\overline{\lambda}$ is an eigenvalue of $T^*$. So:
    \begin{equation*}
        \lambda x = T(x) = T^*(x) = \overline{\lambda} x
    \end{equation*}
    So $\lambda = \overline{\lambda}$, and $\lambda$ is real.
    
    For a orthonormal basis $\beta$, $A = [T]_\beta$ is self-adjoint because $A^* = ([T]_\beta)^* = [T^*]_\beta = [T]_\beta = A$. Define $L_A(x) = Ax$ in $\mathcal{C}^n$. Here we create a function in $\mathcal{C}^n$ from a function in $\mathcal{R}^n$. Let $\gamma$ be the standard basis for $\mathcal{C}$ which is orthonormal. $[L_A]_\gamma = A$ is self-adjoint, so $L_A$ is self-adjoint in $\mathcal{C}^n$. The characteristic polynomial of $L_A$ splits. Since $L_A$ is self-adjoint, all eigenvalues are real, so the polynomial split in $\mathcal{R}$. But $L_A$, $A$ and $T$ has the same characteristic polynomial.
\end{proof}

\begin{theorem}
    Let $T$ be a linear operator on finite dimensional real inner product space. $T$ is self-adjoint if and only if there exists an orthonormal basis $\beta$ for $V$ consisting of eigenvectors of $T$.    
\end{theorem}
\begin{proof}
    By Theorem (\ref{schurincomplexfield}) there exists orthonormal basis $\beta$ for $V$ that $A = [T]_\beta$ is upper triangular. Because $A^* = ([T]_\beta)^* = [T^*]_\beta = [T]_\beta = A$, $A$ is diagonal matrix.
\end{proof}


\begin{theorem}
    For the orthonormal basis of eigenvector $T$ problem we have:
    \begin{enumerate}
        \item If $T$ splits, we have orthonormal basis that make $T$ upper triangular in $\mathcal{R}$ or $\mathcal{C}$. This basis may not be eigenvectors, or $T$ may not have eigenvectors.
        \item $T$ is complex normal.
        \item $T$ is real symmetric.
    \end{enumerate}
\end{theorem}


\begin{theorem}\label{zerotforalltxforselfadjoint}
    Let $T$ be self-adjoint operator. If $\forall x \in V$,$\innerproduct{T(x)}{x} = 0$. Then $T = 0$.\footnote{Self-adjoint is not needed of $V=\mathcal{C}$. See Theorem (\ref{zerotforalltx}) on page \pageref{zerotforalltx}.}
\end{theorem}
\begin{proof}
    Choose orthonormal basis $\beta$ that consist of eigenvector of $T$. For $x\in \beta$, $T(x) =\lambda x$. So
    \begin{equation*}
        0 = \innerproduct{x}{T(x)} = \innerproduct{x}{\lambda x} = \overline{\lambda} \innerproduct{x}{x}
    \end{equation*}
    Hence $\overline{\lambda} = 0$ and $\forall x \in \beta,  T(x) = 0$.
\end{proof}



% unitary and orthogonal operator
\subsection{Unitary and Orthogonal Operator}

\begin{definition}
    Let $T$ be a linear operator on finite dimensional inner product space $V$ over $F$. If $\forall x \in V$,$\norm{T(x)} = \norm{x}$, we call $T$ \cindex{unitary operator} if $F = \mathcal{C}$ or \cindex{orthogonal operator} if $F=\mathcal{R}$.
\end{definition}

\begin{definition}
    A square matrix $A$ is called \cindex{unitary matrix} if $AA^* = A^*A = I$ and \cindex{orthogonal matrix} if $AA^\top = A^\top A = I$.
\end{definition}

\begin{theorem}\label{unitaryproperty}
    Let $T$ be an linear operator. Then the following are equivalent:
    \begin{enumerate}
        \item $TT^* = T^* T = I$.\label{unitaryisnormal}
        \item $\innerproduct{T(x)}{T(y)} = \innerproduct{x}{y}$
        \item If $\beta$ is an orthonormal basis for $V$. Then $T(\beta)$ is an orthonormal basis.
        \item $\norm{T(x)} = \norm{x}$
    \end{enumerate}
    
    So unitary or orthogonal operator preserve inner product and norm.
\end{theorem}
\begin{proof}
    $\innerproduct{x}{y} = \innerproduct{T^* T x}{y} = \innerproduct{T(x)}{T(y)}$.
    
    If $\beta = \{v_1,v_2,\dots,v_n \}$ is an orthonormal basis. $\innerproduct{T(v_i)}{T(v_j)} = \innerproduct{v_i}{v_j} = 0$.
    
    If $\beta$ and $T(\beta)$ are both orthonormal basis, expand $\norm{T(x)}$ and $\norm{x}$ to prove they are equal.
    
    $\innerproduct{x}{x} = \norm{x}^2 = \norm{T(x)}^2 = \innerproduct{T(x)}{T(x)} = \innerproduct{x}{T^*Tx}$. So $\forall x \in V, \innerproduct{x}{(I - T^*T)(x)} = 0$. $I - T^*T$ is normal, so according to Theorem (\ref{zerotforalltxforselfadjoint}), $I - T^* T = 0$.
\end{proof}

\begin{theorem}
    Unitary operator is normal.    
\end{theorem}
\begin{proof}
    See Theorem (\ref{unitaryproperty}) property (\ref{unitaryisnormal}).
\end{proof}



\begin{theorem}
    Let $T$ be a linear operator on \emph{real} inner product space $V$. $V$ has an orthonormal basis of eigenvectors of $T$ with absolute value of all eigenvalues equal to $1$ if and only if $T$ is self-adjoint and orthogonal.    
\end{theorem}
\begin{proof}
    If $T$ is self-adjoint, there is orthonormal basis $\beta$ of eigenvectors. If $T$ is orthogonal, $\forall v_i \in \beta$, $\absolutevalue{\lambda_i} \times \norm{v_i} = \norm{\lambda_i v_i} = \norm{T(v_i)} = \norm{v_i}$, so $\absolutevalue{\lambda_i} = 1$.
    
    If $V$ has orthonormal basis $\beta$ of eigenvectors, $T$ is self-adjoint. $\forall v_i \in \beta$, we have $TT^* (v_i) = T(\lambda_i v_i ) = \lambda_i T(v_i) = \lambda_i^2 v_i$. If $\absolutevalue{\lambda_i} = 1$, $TT^* = I$.
\end{proof}

\begin{theorem}
    Let $T$ be a linear operator on \emph{complex} inner product space $V$. $V$ has an orthonormal basis of eigenvectors of $T$ with absolute value of all eigenvalues equal to  $1$ if and only if $T$ is unitary.
\end{theorem}
\begin{proof}
    If $T$ is unitary, it is normal, so there is orthonormal basis $\beta$ of eigenvectors. If $T$ is unitary, $\forall v_i \in \beta$, $\absolutevalue{\lambda_i} \times \norm{v_i} = \norm{\lambda_i v_i} = \norm{T(v_i)} = \norm{v_i}$, so $\absolutevalue{\lambda_i} = 1$.
    
    If $V$ has orthonormal basis $\beta$ of eigenvectors, $T$ is normal. If $\absolutevalue{\lambda_i} = 1$, $\forall v_i \in \beta$, $\absolutevalue{\lambda_i} \times \norm{v_i} = \norm{\lambda_i v_i} = \norm{T(v_i)} = \norm{v_i}$, so $\norm{T(v_i)} = \norm{v_i}$ and it is unitary.
\end{proof}

\begin{theorem}
    $T$ is unitary or orthogonal if $[T]_\beta$ is unitary or orthogonal for a orthonormal basis $\beta$ of $V$.
\end{theorem}

\begin{definition}
    $A$ is \cindex{unitarily equivalent} or \cindex{orthogonally equivalent} to $D$ if and only if there exists a unitary or orthogonal matrix $P$ that $A = P^* D P$.
\end{definition}

\begin{theorem}
    Let $A$ be a complex square matrix. $A$ is normal if and only if it is unitarily equivalent to a diagonal matrix.    
\end{theorem}

\begin{theorem}
    Let $A$ be a real square matrix. $A$ is symmetric if and only if it is orthogonally equivalent to a diagonal matrix.    
\end{theorem}





% spectral theorem
\subsection{Spectral Theorem}

\begin{definition}
    Let $V = W_1 \oplus W_2$. $T$ is a \cindex{projection} on $W_1$ along $W_2$ if $\forall x = x_1 + x_2$ that $x_1 \in W_1$ and $x_2 \in W_2$, $T(x) = x_1$.
\end{definition}

\begin{theorem}
    $T$ is a projection if and only if $T^2 = T$.
\end{theorem}

\begin{definition}
    $T$ is an \cindex{orthogonal projection} if $\rangespace{T}^\bot = \nullspace{T}$ and $\rangespace{T}= \nullspace{T}^\bot$\footnote{In finite dimensional space $V$, $\rangespace{T}^\bot = \nullspace{T} \leftrightarrow \rangespace{T}= \nullspace{T}^\bot$}. 
\end{definition}



\begin{theorem}
    $T$ is an orthogonal projection if and only if $T$ has an adjoint $T^*$ that $T^2 = T = T^*$.
\end{theorem}
\begin{proof}
    $T^2 = T$ because $T$ is a projection. Let $x=x_1+x+2$ and $y=y_1+y_2$ where $x_1,y_1 \in \rangespace{T}$ and $x_2,y_2 \in \nullspace{T}$. So
    \begin{equation*}
        \begin{aligned}
            \innerproduct{x}{T(y)} &= \innerproduct{x_1 + x_2}{y_1} = \innerproduct{x_1}{y_1} \\
            \innerproduct{T(x)}{y} &= \innerproduct{x_1}{y_1 + y_2} = \innerproduct{x_1}{y_1}
        \end{aligned}
    \end{equation*}
    So $T = T^*$ and $T^2 = T = T^*$.
    
    For the reverse side, prove that $\rangespace{T}^\bot = \nullspace{T}$ and $\rangespace{T}= \nullspace{T}^\bot$.
\end{proof}

\begin{theorem}[\cindex{Spectral Theorem}]
    Let $T$ be real symmetric or complex normal with distinct eigenvalue $\lambda_i$ and its corresponding eigenspace $W_i$. Let $T_i$ be the orthogonal projection on $W_i$. We have:
    \begin{enumerate}
        \item $T_i T_j = \delta_ij T_i$.
        \item $\displaystyle I = \sum_{i=1}^k T_i$
        \item $\displaystyle T = \sum_{i=1}^k \lambda_i T_i$
    \end{enumerate}
    
    $\lambda_i$ is the \cindex{spectrum} of $T$. $I$ is the resolution of the identity operator induced by T. $\displaystyle T = \sum_{i=1}^k \lambda_i T_i$ is the \cindex{spectral decomposition} of $T$.
\end{theorem}
\begin{proof}
    Let $\displaystyle x= \sum_{i=1}^k x_i$ where $x_i \in W_i$. Then
    \begin{equation*}
        T(x) = \sum_{i=1}^k T(x_i) = \sum_{i=1}^k \lambda_i x_i= \sum_{i=1}^k \lambda_i T_i (x_i) = \sum_{i=1}^k \lambda_i T_i (x) = \left(\sum_{i=1}^k \lambda_i T_i \right) x
    \end{equation*}
\end{proof}

\begin{theorem}
    Let $F=\mathcal{C}$. $T$ is normal if and only if $\exists g \in P$, $T^* = g(T)$.
\end{theorem}
\begin{proof}
    Let $\displaystyle T = \sum_{i=1}^k \lambda_i T_i$ be the spectral decomposition of $T$. Take the adjoint of both side and we have
    \begin{equation}
        T^* = \sum_{i=1}^k \overline{\lambda_i} T_i^*
    \end{equation}
    According to Lagrange formula\footnote{Theorem (\ref{lagrangeinterpolationformula}) on page \pageref{lagrangeinterpolationformula}.} , $\exists g$, $g(\lambda_i) = \overline{\lambda_i}$. So $g(T) = T^*$. The reverse is easy to prove.
\end{proof}

\begin{theorem}
    Let $F=\mathcal{C}$. $T$ is unitary if and only if $T$ is normal and $\absolutevalue{\lambda} = 1$ for all eigenvalue $\lambda$ of $T$.
\end{theorem}
\begin{proof}
    Let $\displaystyle T = \sum_{i=1}^k \lambda_i T_i$ be the spectral decomposition of $T$. We have
    \begin{equation*}
    TT^* = \left( \sum_{i=1}^k \lambda_i T_i  \right) \times \left( \sum_{i=1}^k \overline{\lambda_i} T_i \right) = \sum_{i=1}^k \absolutevalue{\lambda_i}^2 T_i^2 = \sum_{i=1}^k \absolutevalue{\lambda_i}^2 T_i = \sum_{i=1}^k T_i  = I    
    \end{equation*}
\end{proof}


\begin{theorem}
    Let $F=\mathcal{C}$ and $T$ normal. $T$ is self-adjoint if and only if every eigenvalue of $T$ is real.    
\end{theorem}
\begin{proof}
    $\displaystyle T^* = \sum_{i=1}^k \overline{\lambda_i} T_i = \sum_{i=1}^k \lambda_i T_i = T$, so $\overline{\lambda_i} = \lambda_i$.
\end{proof}



% single value decomposition
\subsection{Single Value Decomposition}

\begin{theorem}
    Let $T:V \rightarrow W$ be a linear transformation with rank $r$. Then there exists orthonormal basis $\beta = \{v_1, v_2, \dots, v_n \}$ for $V$ and $\gamma = \{ u_1, u_2, \dots, u_m \}$ for $W$ and positive scalars \cindex{singular values} $\sigma_1 \geq \sigma_2 \geq \dots \geq \sigma_r$ such that
    \begin{equation}
        T(v_i) = \begin{cases}
            \sigma_i u_i & \text{if } 1 \leq i \leq r \\
            0 & \text{if } i > r
        \end{cases}
    \end{equation}
    
    Conversely, for $1 \leq i \leq n$, $v_i$ is an eigenvector of $T^*T$ with corresponding eigenvalue $\sigma_i^2$ if $1 \leq i \leq r$ and $0$ if $i > r$. 
\end{theorem}
\begin{proof}
    $T^*T$ is a positive semidefinite linear operator of rank $r$ on $V$. So there is an orthonormal basis $v_i$ for $V$ consisting of eigenvectors of $T^*T$ with corresponding eigenvalues $\lambda_i$ where $\lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_r > 0$ and $\lambda_i = 0$ for $i > r$. For $1 \leq i \leq r$, define $\sigma_i = \sqrt{\lambda_i}$ and $\displaystyle u_i = \frac{1}{\sigma_i} T(v_i)$. We have:
    \begin{equation*}
    \innerproduct{u_i}{u_j} = \innerproduct{\frac{1}{\sigma_i} T(v_i)}{\frac{1}{\sigma_j} T(v_j)} = \frac{1}{\sigma_i \sigma_j} \innerproduct{T^*T(v_i)}{v_j} = \frac{1}{\sigma_i \sigma_j} \innerproduct{\lambda_i v_i}{v_j} = \frac{\sigma_i^2}{\sigma_i \sigma_j} \innerproduct{v_i}{v_j} = \delta_{ij}
    \end{equation*}
    
    So $\{u_1, u_2, \dots, u_r \}$ are orthonormal. Extend it to an orthonormal basis $\{u_1, u_2, \dots, u_m \}$
\end{proof}

\begin{definition}
    The \cindex{singular values} of $A$ is the singular value of $L_A$.
\end{definition}

\begin{theorem}[Singular Value Decomposition Theorem]
    Let $A_{m \times n}$ be of rank $r$ with positive singular values $\sigma_1 \geq \sigma_2 \geq \dots \geq \sigma_r$, and let $\Sigma_{m \times n}$ be
    \begin{equation}
        \Sigma_{ij} = \begin{cases}
            \sigma_i & \text{if } i = j \leq r \\
            0
        \end{cases}
    \end{equation}
    Then there exists \cindex{singular value decomposition} that with $U_{m \times m}$ and $V_{n \times n}$, we have
    \begin{equation}
        A = U \Sigma V^*
    \end{equation}
    
    The process to find singular value decomposition is:
    \begin{enumerate}
        \item find singular value of $A$ by calculating the eigenvalue of $A^*A$.
        \item sort the singular value from big to small.
        \item for non-zero singular value $\sigma_i$, put $\sqrt{\sigma_i}$ to the $i$-th diagonal of $\Sigma$.
        \item form $U$ of normalized eigenvector of $A^*A$.
        \item for non-zero singular value $\sigma_i$, calculate orthonormal vector $\displaystyle u_i = \frac{1}{\sigma_i} L_A(v_i)$.
        \item expand the $u_i$ to orthonormal basis and form $V$.
    \end{enumerate}
\end{theorem}



% Polar Decomposition
\subsection{Polar Decomposition}

\begin{theorem}[Polar Decomposition]
    Any square matrix $A$, there exists a \cindex{Polar Decomposition} using unitary matrix $W$ and a positive semidefinite matrix $P$ that 
    \begin{equation}
        A = WP
    \end{equation}
    If $A$ is invertible, the Polar Decomposition is unique.
\end{theorem}
\begin{proof}
    Use singular value decomposition on $A$ and we get $A = U \Sigma V^* = U V^* V \Sigma V^* = ( U V^*) ( V \Sigma V^*) =  WP$.
    So let $W = U V^*$ and $P = V \Sigma V^*$.
\end{proof}



% Pseudoinverse
\subsection{Pseudoinverse}

\begin{definition}
    Let $T: V \rightarrow W$ be a linear transformation. Let $L: \nullspace{T}^\bot \rightarrow \rangespace{T}$ be a linear transformation that $\forall x \in \nullspace{T}^\bot$, $L(x) = T(x)$. The \cindex{pseudoinverse} (or \cindex{Moore-Penrose generalised inverse}) of $T$ is a unique linear transformation from $W$ to $V$ that
    \begin{equation}
        T^\dag (y) = \begin{cases}
            L^{-1}(y) & \text{for } y \in \rangespace{T} \\
            0 & \text{for } y \in \rangespace{T}^\bot
        \end{cases}
    \end{equation}
    
    So although not all $T$ has inverse, the restriction $\left. T \right|_{\nullspace{T}^\bot}$ could have proper inverse.
\end{definition}

\begin{theorem}
    Let $A_{m \times n}$ be a square matrix of rank $r$ with singular value decomposition $A = U \Sigma V^*$ and non-zero singular values $\sigma_1 \geq \sigma_2 \geq \dots \geq \sigma_r$. Let $\Sigma^\dag_{m \times n}$  be a matrix that 
    \begin{equation}
        \Sigma^\dag_{ij} = \begin{cases}
            \frac{1}{\sigma_i} & \text{if } i = i \leq r \\
            0 
        \end{cases}
    \end{equation}
    Then $A^\dag = V \Sigma^\dag U^*$ is a singular value decomposition of $A$.
\end{theorem}


\begin{theorem}
    Let $T: V \rightarrow W$ be a linear transformation, then
    \begin{enumerate}
        \item $T^\dag T$ is the orthogonal projection of $V$ on $\nullspace{T}^\bot$.
        \item $TT^\dag$ is the orthogonal projection of $W$ on $\rangespace{T}$.
    \end{enumerate}    
\end{theorem}


\begin{theorem}
    For a system of linear equations $Ax = b$. If $z = A^\dag b$, then
    \begin{enumerate}
        \item If $Ax=b$ is consistent, then $z$ is the unique solution with minimal norm.
        \item If $Ax=b$ is inconsistent, then $z$ is the best approximation: $\forall y$, $\norm{Ax-b} \leq \norm{Ay-b}$. Also if $Az = Ay$, then $\norm{z} \leq \norm{y}$.
    \end{enumerate}
\end{theorem}





% conditioning
\subsection{Conditioning}

\begin{definition}
    For $Ax=b$, if a small change to $A$ and $b$ cause small change to $x$, the property is called \cindex{well-conditioned}. Otherwise the system is \cindex{ill-conditioned}.
\end{definition}

\begin{definition}
    The \cindex{relative change} in $b$ is $\displaystyle \frac{\norm{\dif{b}}}{\norm{b}}$ with $\norm{\cdot}$ be the standard norm on $\mathcal{C}^n$.
\end{definition}

\begin{definition}
    The \cindex{Euclidean norm} of square matrix $A$ is 
    \begin{equation}
        \norm{A} = \max_{x \neq 0} \frac{\norm{Ax}}{\norm{x}}
    \end{equation}
\end{definition}


\begin{definition}
    Let $B$ be a self-adjoint matrix. The \cindex{Rayleigh quotient} for $x \neq 0$ is $\displaystyle R(x) = \frac{\innerproduct{Bx}{x}}{\norm{x}^2}$
\end{definition}


\begin{theorem}
For a self-adjoint matrix $B$, the $\displaystyle \max_{x \neq 0} R(x)$ is the largest eigenvalue of $B$ and $\displaystyle \min_{x \neq 0} R(x)$ is the smallest eigenvalue of $B$.
\end{theorem}
\begin{proof}
    Choose the orthonormal basis $v_i$ of $B$ such that $Bv_i = \lambda_i v_i$ where $\lambda_1 \geq \lambda_2 \geq \lambda_n$. $\forall x \in F^n$, $\exists a_i$ that $\displaystyle x = \sum_{i=1}^n a_i v_i$. So
    \begin{equation*}
        R(x) = \frac{\innerproduct{Bx}{x}}{\norm{x}^2} = \frac{\innerproduct{\sum_{i=1}^n a_i \lambda_i v_i}{\sum_{j=1}^n a_j v_j}}{\norm{x}^2} = \frac{\sum_{i=1}^n \lambda_i \absolutevalue{a_i}^2}{\norm{x}^2} \leq \frac{\lambda_1 \sum_{i=1}^n \absolutevalue{a_i}^2}{\norm{x}^2} = \frac{\lambda_1 \norm{x}^2}{\norm{x}^2} = \lambda_1
    \end{equation*}
\end{proof}

\begin{theorem}
    $\norm{A} = \sqrt{\lambda}$ where $\lambda$ is the largest eigenvalue of $A^* A$.
\end{theorem}

\begin{theorem}
    $\lambda$ is an eigenvalue of $A^* A$ if and only if $\lambda$ is an eigenvalue of $AA^*$.
\end{theorem}

\begin{theorem}
    Let $A$ be invertible matrix. Then $\displaystyle \norm{A^{-1}} = \frac{1}{\sqrt{\lambda}}$ where $\lambda$ is the smallest eigenvalue of $A^*A$.
\end{theorem}

\begin{definition}
    $\norm{A} \times \norm{A^{-1}}$ is the \cindex{condition number} of $A$ and denoted as $\text{cond}(A)$.
\end{definition}

\begin{theorem}
    For system $Ax=b$ where $A$ is invertible and $b \neq 0$, we have:
    \begin{enumerate}
        \item For any norm $\norm{\cdot}$, we have $\displaystyle \frac{1}{\text{cond}(A)} \frac{\norm{\dif b}}{\norm{b}} \leq \frac{\norm{\dif x}}{\norm{x}} \leq \text{cond}(A) \frac{\norm{\dif b}}{\norm{b}}$.
        \item If $\norm{\cdot}$ is the Euclidean norm, then $\displaystyle \text{cond}(A) = \sqrt{\frac{\lambda_1}{\lambda_n}}$ where $\lambda_1$ and $\lambda_n$ are the largest and smallest eigenvalue of $A^*A$.
    \end{enumerate}
\end{theorem}


























































































































































