\section{Vector Space}

\subsection{Field}

\begin{definition}
    For $0$ and $1$ of a field $F$, the smallest $n$ that $\displaystyle \sum_{i=1}^n 1 = 0$ is called the \cindex{characteristic} of $F$. If no such $n$ exists, $F$ is called \cindex{characteristic zero}.
    \qed
\end{definition}

\begin{definition}
    The field $Z_2$ consists of two elements $0$ and $1$:
    \begin{itemize}
        \item $0 + 0 = 0$
        \item $0 + 1 = 1 + 0 = 1$
        \item $1 + 1 = 0$
        \item $0 \times 0 = 0$
        \item $0 \times 1 = 1 \times 0 = 0$
        \item $1 \times 1 = 1$
    \end{itemize}
\end{definition}

\subsection{Vector}

Algebra is concerned with how to manipulate symbolic combinations of object and how to equate one with another.

\begin{definition}
A \cindex{vector space} vector space $V$ over a \cindex{field} field $F$ has two operation $\{+,\times\}$ with $\vec{0}$ and $1$. \qed
\end{definition}

One vector space example is tuple. which is often used to count several types of objects. Scalar field is a vector space as well, but not very interesting.


\begin{definition}
	A \cindex{subspace} is a subset $W$ of vector space $V$ that is closed under $\{+,\times\}$.
\end{definition}

\begin{theorem}
    $\{0\}$ is a subspace of all vector space.    
\end{theorem}


When we say a subset is a subspace of a vector space, we mean it is a vector space as well.

\begin{definition}
	a \cindex{trace} of an $n \times n$ matrix $M$, denoted $\text{tr}(M)$, is the sum of diagonal entries:
	\begin{equation}
		\text{tr}(M) = \sum_{i=1}^n M_{ii}
	\end{equation}
\end{definition}

\begin{definition}
	A \cindex{span} of a nonempty subset $S$ of a vector space $V$ is the set consisting of all linear combinations of the vectors in $S$. If $\text{span}(S) =V$, $S$ \cindex{generate} (or span) $V$.
	\qed
\end{definition}

\begin{definition}
    The span of $\emptyset$ is $\{0\}$, not $\emptyset$.
\end{definition}

A span set is useful because it allow one to describe all vectors in terms of a much smaller space.

\begin{definition}
	A subset $S$ of $V$ is \cindex{linearly dependent} if there exist a finite number of distinct vector $u_1, u_2, \dots, u_n$ in $S$ and scalars $a_1, a_2, \dots, a_n$, not all $0$, that:
	\begin{equation}
		\sum_{i=1}^n a_i u_i = 0
	\end{equation}
	
	$S$ is called \cindex{linearly independent} if it is not linearly dependent. $\emptyset$ is linearly independent.
	
	\qed
\end{definition}

\begin{theorem}
	Let $S$ be linearly independent, $v$ is not in $S$. Then $S \cup {v} $ is linearly dependent if $v \in \text{span}(S)$.
\end{theorem}

\subsection{Basis}

\begin{definition}
	A \cindex{basis} $\beta$ for $V$ is a linearly independent subset of $V$ that generate $V$. 
	\qed
\end{definition}

A vector space is usually infinite. It is desirable to describe this infinite set using a finite subset, which is  the role of basis.

\begin{theorem}
    $\emptyset$ is a basis for zero vector space $\{0\}$, so every vector space has a basis.
\end{theorem}

\begin{definition}
    The \cindex{standard basis for $F^n$} is $e_1=(1,0,0,\dots,0)$, $e_2=(0,1,0,\dots,0)$, $e_n=(0,0,\dots,1)$.
\end{definition}

\begin{definition}
    The \cindex{standard basis for $P_n(F)$} is $\{1,x,x^2,\dots,x^n\}$.
\end{definition}


\begin{theorem}
	$\beta$ is a basis of $V$ if $\forall v \in V $, $v$ has a unique representation as a linear combination of vectors of $\beta$.
\end{theorem}

\begin{theorem}
    A finite spanning set for $V$ can be reduced to a basis.    
\end{theorem}


\begin{theorem}[\cindex{Replacement Theorem}]
	Let $V$ be generated by a set $G$ with $n$ vectors. Let $L$ be a linearly independent subset of $V$ with $m$ vectors. Then $m < n$ and $\exists H \subset G$ with $n-m$ vectors such that $L \cup H$ generate $V$.
	\qed
\end{theorem}

\begin{theorem}
    Let $V$ have a finite basis. Then every basis contains the same number of vectors. This number is an intrinsic property of $V$ and called the \cindex{dimension} of $V$.    
\end{theorem}

\begin{theorem}
    Let $V$ be a vector space with dimension $n$:
    \begin{itemize}
        \item any finite generating set for $V$ contains at least $n$ vectors. If they contains exactly $n$ vectors, they are a basis.
        \item any linearly independent subset of $n$ vectors is a basis.
        \item every linearly independent subset could be extended to a basis.
    \end{itemize}    
\end{theorem}




\begin{definition}[\cindex{Lagrange Interpolation Formula}]
let ${c_0, c_1, \dots, c_n}$ be distinct scalars in field $F$. Define  $n+1$ function $\{f_i \}$ as:
	\begin{equation}
	f_i(x) = \prod_{k=0, k \neq i}^n \frac{x - c_k}{c_i - c_k}
\end{equation}
then $\beta = \{f_i\}$ is a basis of $\mathbb{P}_n(F)$, where \cindex{$\mathbb{P}_n(F)$} is a set of all polynomials over $F$. For $\forall g \in \mathbb{P}_n(F)$, we have
	\begin{equation}
		g = \sum_{i=0}^n g(c_i) f_i
	\end{equation}
	
	To generate a $g$ of degree $n$ that passes $n+1$ points $(x_i, y_i)$, first use $\{x_i\}$ to generate $\{f_i \}$, then $g = \sum\limits_{i=0}^n y_i f_i $
\end{definition}


\begin{proof}
	since $\beta$ is a basis of $\mathbb{P}_n(F)$, $\forall g \in \mathbb{P}_n(F)$,
	\begin{equation*}
		g = \sum_{i=0}^n b_i f_i
	\end{equation*}
	it follows that
	\begin{equation*}
		g(c_j) = \sum_{i=0}^n b_i f_i(c_j) = b_j
	\end{equation*}

	so $g = \sum\limits_{i=0}^n g(c_i) f_i$.
\end{proof}


\begin{theorem}
for any two subspace $W_1$ and $W_2$ of $V$, their dimension has a relation:
\begin{equation}
	\text{dim}(W_1 + W_2) = \text{dim}(W_1) + \text{dim}(W_2) - \text{dim}(W_1 \cap W_2)
\end{equation}
\end{theorem}


\begin{definition}
    here are the definition of common terms:
    
    \cindex{square matrix}: a matrix $M$ that $i = j$. It is usually denoted as $M$, not $A$.
        
    \cindex{zero vector}: $\vec{0}$.
    
    \cindex{transpose}: $(A^\top)_{ij} = A_{ji}$
    
    \cindex{symmetric matrix}: $A^\top = A$.

    \cindex{diagonal matrix}: for a $n \times n$ square matrix $M$ that $M_{ij} = 0$ if $i \neq j$.
    
    \cindex{upper triangular}: $A_{ij} = 0$ if $i > j$.
    \qed    
\end{definition}

\begin{definition}
    Let $F$ be a family of sets. A member $M$ of $F$ is called \cindex{maximal} if $M$ is contained in no member of $F$ other than $M$ itself.
\end{definition}

\begin{definition}
    A collection of set $C$ is called a \cindex{chain} if for each pair of sets $A$ and $B$ in $C$, either $A \subseteq B$ or $B \subseteq A$.
\end{definition}

\begin{theorem}
    Let $F$ be a family of sets. If for each chain $C \subseteq F$, there exists a member of $F$ that contains each member of $C$, then $F$ contains a maximal member.    
\end{theorem}

\begin{proof}
    use axiom of choice. Note that the maximal member may not be in $C$.
\end{proof}

\begin{definition}
    Let $S$ be a subset of a vector space $V$. A \cindex{maximal linearly independent subset} of $S$ is a subset $B$ of $S$ that:
    \begin{enumerate}
        \item $B$ is linearly independent.
        \item The only linearly independent subset of $S$ that contains $B$ is $B$.
    \end{enumerate}
\end{definition}

\begin{theorem}
    If $V$ has a basis $\beta$, $\beta$ is maximal linearly independent.
\end{theorem}
\begin{proof}
    A basis is linearly independent. Because a basis generate $V$, nothing could be added to it and still make it linearly independent.
\end{proof}



\begin{theorem}
    Let $V$ be a vector space and $S$ a subset that generate $V$. If $\beta$ is a maximal linearly independent subset of $S$, then $\beta$ is a basis $V$.    
\end{theorem}
\begin{proof}
    $\beta$ is linearly independent, so only need to prove that $\beta$ generate $V$. It is easy because $\beta$ is maximal in $S$ so nothing from $S$ could be added to it.
\end{proof}

\begin{theorem}
    Let $S$ be a linearly independent subset of a vector space $V$. There exists a maximal linearly independent subset of $V$ that contains $S$.    
\end{theorem}
\begin{proof}
    Let $F$ be a family of all linearly independent subsets of $V$ that contains $S$. For a chain $C$ in $F$, let $U$ be the union of all its member. This $U$ is linearly independent and belongs to $F$, so it is a maximal linearly independent subset of $F$, which is a basis of $F$.
\end{proof}


\begin{theorem}
    Every vector space has a basis.    
\end{theorem}




