\section{Vector Space}

Algebra is concerned with how to manipulate symbolic combinations of object and how to equate one with another.

\begin{definition}
A \cindex{vector space} vector space $V$ over a \cindex{field} field $F$ has two operation $\{+,\times\}$ with $\vec{0}$ and $1$.
\end{definition}

One vector space example is tuple. which is often used to count several types of objects. Scalar field is a vector space as well, but not very interesting.


\begin{definition}
	A \cindex{subspace} is a subset $W$ of vector space $V$ that is closed under $\{+,\times\}$.
\end{definition}

\begin{definition}
	a \cindex{trace} of an $n \times n$ matrix $M$, denoted $\text{tr}(M)$, is the sum of diagonal entries:
	\begin{equation}
		\text{tr}(M) = \sum_{i=1}^n M_{ii}
	\end{equation}
\end{definition}

\begin{definition}
	A \cindex{span} of a nonempty subset $S$ of a vector space $V$ is the set consisting of all linear combinations of the vectors in $S$. especially $\text{span}(\emptyset) = \{ \emptyset \}$. If $\text{span}(S) =V$, $S$ \cindex{generate} (or span) $V$.
\end{definition}

A span set is useful because it allow one to describe all vectors in terms of a much smaller space.

\begin{definition}
	A subset $S$ of $V$ is \cindex{linearly dependent} if there exist a finite number of distinct vector $u_1, u_2, \dots, u_n$ in $S$ and scalars $a_1, a_2, \dots, a_n$, not all $0$, that:
	\begin{equation}
		\sum_{i=1}^n a_i u_i = 0
	\end{equation}
	
	$S$ is called \cindex{linearly independent} if it is not linearly dependent. $\emptyset$ is linearly independent.
\end{definition}

\begin{theorem}
	Let $S$ be linearly independent, $v$ is not in $S$. Then $S \cup {v} $ is linearly dependent if $v \in \text{span}(S)$
\end{theorem}

\begin{definition}
	A \cindex{basis} $\beta$ for $V$ is a linearly independent subset of $V$ that generate $V$.
\end{definition}

A basis is a small subset of vectors which could be used to describe all other vectors in a vector space.

\begin{theorem}
	$\beta$ is a basis of $V$ if $\forall v \in V $, $v$ has a unique representation as a linear combination of vectors of $\beta$.
\end{theorem}

\begin{theorem}[\cindex{Replacement Theorem}]
	Let $V$ be generated by a set $G$ with $n$ vectors. Let $L$ be a linearly independent subset of $V$ with $m$ vectors. Then $m < n$ and $\exists H \subset G$ with $n-m$ vectors such that $L \cup H$ generate $V$.
\end{theorem}

It could prove that all finite basis has the same number of vectors, which is the \cindex{dimension} of vector space.

\begin{definition}[\cindex{Lagrange Interpolation Formula}]
let ${c_0, c_1, \dots, c_n}$ be distinct scalars in field $F$. Define  $n+1$ function $\{f_i \}$ as:
	\begin{equation}
	f_i(x) = \prod_{k=0, k \neq i}^n \frac{x - c_k}{c_i - c_k}
\end{equation}
then $\beta = \{f_i\}$ is a basis of $\mathbb{P}_n(F)$, where \cindex{$\mathbb{P}_n(F)$} is a set of all polynomials over $F$. For $\forall g \in \mathbb{P}_n(F)$, we have
	\begin{equation}
		g = \sum_{i=0}^n g(c_i) f_i
	\end{equation}
	
	To generate a $g$ of degree $n$ that passes $n+1$ points $(x_i, y_i)$, first use $\{x_i\}$ to generate $\{f_i \}$, then $g = \sum\limits_{i=0}^n y_i f_i $
\end{definition}


\begin{proof}
	since $\beta$ is a basis of $\mathbb{P}_n(F)$, $\forall g \in \mathbb{P}_n(F)$,
	\begin{equation*}
		g = \sum_{i=0}^n b_i f_i
	\end{equation*}
	it follows that
	\begin{equation*}
		g(c_j) = \sum_{i=0}^n b_i f_i(c_j) = b_j
	\end{equation*}

	so $g = \sum\limits_{i=0}^n g(c_i) f_i$.
\end{proof}


\begin{theorem}
for any two subspace $W_1$ and $W_2$ of $V$, their dimension has a relation:
\begin{equation}
	\text{dim}(W_1 + W_2) = \text{dim}(W_1) + \text{dim}(W_2) - \text{dim}(W_1 \cap W_2)
\end{equation}
\end{theorem}

