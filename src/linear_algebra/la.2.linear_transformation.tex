\section{Linear Transformation and Matrix}

\subsection{Linear Transformation}


\begin{definition}
	A \cindex{linear transformation}  from $V$ to $W$ is a function $T: V \rightarrow W$ that:
	\begin{enumerate}
		\item $T(x+y) = T(x) + T(y)$
		\item $T(c x) = c T(x)$
	\end{enumerate}
\end{definition}

The two linear transformation verification criteria could be combined into one: prove that 
\begin{equation}
    T(cx + y) = cTx+Ty
\end{equation}


The \cindex{identity transformation}  $\mathrm{I}_v : V \rightarrow V$ is defined as $\mathrm{I}_v(x) = x$.

The \cindex{zero transformation}  $T_0: V \rightarrow W$ is defined as $T_0 = 0$.

\begin{definition}
	Let $T:V \rightarrow W$ be linear. the \cindex{null space}  $\nullspace{T}$ of $T$ is the set $\set{x \in V : T(x) = 0}$. It is also called the \cindex{kernel} of $T$. It measures how much  information is lost by the transformation $T$.
\end{definition}

\begin{definition}
	The \cindex{range}  of $T$ is defined as $\rangespace{T} = \set{ {T(x):x \in V}}$. It measures how much information is retained by the transformation $T$.
\end{definition}

\begin{theorem}
	Let $T: V \rightarrow W$ be linear. If $\beta=\set{v_i}$ is a basis for $V$, then
	\begin{equation}
		\rangespace{T} =  \vectorspan{T(\beta)} = \vectorspan{\set{T(v_i)}}
	\end{equation}
\end{theorem}

\begin{definition}
	Let $T: V \rightarrow W$ be linear. the \cindex{nullity}  of $T$ is the dimension of $\nullspace{T}$. The \cindex{rank} \label{rankdefinition} of $T$ is the dimension of $\rangespace{T}$.
\end{definition}

\begin{theorem}[\cindex{Dimension Theorem}]
	If $V$ is finite dimensional, $T:V\rightarrow W$ is linear, then
	\begin{equation}
		\dimension{\nullspace{T}} + \dimension{\rangespace{T}} = \dimension{T}
	\end{equation}
\end{theorem}

\begin{proof}
	expand nullity set to a basis and prove the image of extra parameters are independent.
\end{proof}

\begin{theorem}\label{uniquelineartransformation}
	Let $V:\set{v_i}$ and $W:\set{w_i}$ be vector space over $F$, and their dimensions are the same. Then there exists a unique linear transformation $T:V \rightarrow W$ such that $T(v_i) = w_i$.
\end{theorem}

\begin{proof}
    For $\displaystyle x = \sum_{i=1}^{n} a_i v_i$, define $T:V \rightarrow W$ that $\displaystyle T(x) = \sum_{i=1}^n a_i w_i$.
\end{proof}

\thmref{uniquelineartransformation} is useful when proving two functions are the same.


\begin{theorem}
    Let $T: V \rightarrow W$ be a linear transformation. $T$ is one-to-one if and only if $\nullspace{T} = \{ 0 \}$.    
\end{theorem}







% matrix representation
\subsection{Matrix Representation}

\begin{definition}
	A \cindex{ordered basis}  for $V$ is a basis for $V$ with a specific order.
\end{definition}

\begin{definition}
	$\set{e_1, e_2, \dots, e_n}$ is the \cindex{standard ordered basis}  for $F^n$. $\set{1, x, \dots, x^n}$ is the \cindex{standard ordered basis}  for $P_n (F)$.
\end{definition}

\begin{definition}
	Let $\beta = \set{u_1, u_2, \dots, u_n}$ be an ordered basis for $V$. $\forall x \in V$, let $\set{a_1, a_2, \dots, a_n}$ be the unique scalar such that
	\begin{equation*}
		x = \sum_{i=1}^n a_i u_i
	\end{equation*}
	
	the \cindex{coordinate vector}  of $x$ relative to $\beta$, is defined as 
	\begin{equation}
		\coordinate{x}_\beta = \begin{bmatrix}
		a_1 \\
		a_2 \\
		\vdots \\
		a_n
		\end{bmatrix}
	\end{equation}
	
	Note that $\coordinate{u_i}_\beta = e_i$.
\end{definition}

\begin{definition}
	Let $V$ with ordered basis $\beta=\set{v_i}$, $W$ with ordered basis $\gamma:\set{w_i}$, $T:V \rightarrow W$ be linear. There exists unique scala $a_{ij} \in F$ such that
	\begin{equation}
		T(v_j) = \sum_{i=1}^m a_{ij} w_j
	\end{equation}
	
	The $m \times n$ \cindex{matrix}\footnote{The word matrix is Latin for womb which is the same root as matrimony. The idea is that a matrix is a receptacle for holding numbers. } $A$ defined by $A_{ij}=a_{ij}$ is the \cindex{matrix representation}  of $T$ in the ordered basis $\beta$ and $\gamma$ and write $A=\coordinate{T}_\beta^\gamma$. If $V = W$ and $\beta = \gamma$, we write $A=\coordinate{T}_\beta$.
	\qed
\end{definition}

	Note that the $j$-th column of $A$ is $\coordinate{T(v_j)}_\gamma$: $\coordinate{T}_\beta^\gamma = \coordinate{\dots, \coordinate{T(v_j)}_\gamma, \dots}$.
	
	Note that $T$ is the relationship between two basis. The value of $T$ might be the same as basis, for example when they are operators on $F^n$, but $T$ and basis are different objects. It is easy to confuse them, especially on $F^n$.
	

\begin{theorem}
	If $\mathrm{U},T:V \rightarrow W$ are linear transformation that $\coordinate{\mathrm{U}}_\beta^\gamma = \coordinate{T}_\beta^\gamma$, then $\mathrm{U} = T$.
\end{theorem}

\begin{definition}
	\cindex{$\mathcal{L}(V,W)$} contains all linear transformation from $V$ to $W$.
\end{definition}

\begin{theorem}
	Let $T$,$\mathrm{U}$ be linear transformation over $V$ and $W$, 
	\begin{enumerate}
		\item $\coordinate{T + \mathrm{U}}_\beta^\gamma = \coordinate{T}_\beta^\gamma  + \coordinate{\mathrm{U}}_\beta^\gamma $
		\item $\coordinate{a T }_\beta^\gamma = a \coordinate{T}_\beta^\gamma $ for all scalar $a$
	\end{enumerate}
\end{theorem}

\begin{theorem}
	let $T:V\rightarrow W$ and $\mathrm{U}:W\rightarrow Z$. Then $\mathrm{UT}: V \rightarrow Z$ is linear.
\end{theorem}

\begin{definition}
	Let $T:V\rightarrow W$ and $\mathrm{U}:W\rightarrow Z$ be linear transformation. $A_{m \times n}=\coordinate{\mathrm{U}}_\alpha^\beta$ and $B_{n \times p}=\coordinate{T}_\beta^\gamma$ where $\alpha=\set{v_i}$, $\beta=\set{w_i}$, $\gamma=\set{z_i}$. Define the \cindex{product} of matrix $AB$ as:
	\begin{equation}
		(AB)_{ij} = \sum_{k=1}^n A_{ik} B_{kj}
	\end{equation}
	
	then 
	\begin{equation}
	    \coordinate{UT}_\alpha^\gamma = \coordinate{U}_\beta^\gamma \coordinate{T}_\alpha^\beta
	\end{equation}
\end{definition}

\begin{proof}
	For product $AB=\coordinate{UT}_\alpha^\gamma$, we have 
	\begin{equation}
		\begin{aligned}
			(UT)(v_j) &= U(T(v_j)) = U \left( \sum_{k=1}^m B_{kj} w_k \right) = \sum_{k=1}^m B_{kj} U(w_k) \\
			&= \sum_{k=1}^m B_{kj} \left( \sum_{i=1}^p A_{ik} z_i \right) = \sum_{k=1}^m  \left( \sum_{i=1}^p A_{ik} B_{kj} \right)  z_i \\
			&= \sum_{i=1}^p C_{ij} z_i
		\end{aligned}
	\end{equation}
\end{proof}


\begin{definition}
	the \cindex{Kronecker delta}  $\delta_{ij}$ is defined as 
	\begin{equation}
		\delta_{ij} = \begin{cases}
			1 & \text{, if } i = j \\
			0 & \text{, if } i \neq j
 		\end{cases}
	\end{equation}
\end{definition}

\begin{definition}
	The $n\times n$ \cindex{identity matrix} \cindex{$I_n$} is defined as $\left(I_n \right)_{ij} = \delta_{ij}$.
\end{definition}

\begin{theorem}
	Let $u_j$ and $v_j$ be the $j$th column of $AB$ and $B$, then
	\begin{enumerate}
		\item $u_j = A v_j$ : $AB = \coordinate{A v_1, A v_2, \dots, A v_j, \dots, A v_p}$
		\item $v_j = B e_j$ : $B = B \times I_n$
	\end{enumerate}
\end{theorem}

\begin{theorem} Let $T:V \rightarrow W$ be linear, we have
	\begin{equation}
		\coordinate{T(u)}_\gamma = \coordinate{T}_\beta^\gamma \coordinate{u}_\beta
	\end{equation}
\end{theorem}

\begin{proof}
	Fix $u \in V$, and define linear transformation $f: F \rightarrow V$ by $f(a) = a u$ and $g: F \rightarrow W$ by $g(a) = a T(u)$. Let $a=\{1\}$ be the standard basis of $F$. Notice that $g=Tf$. we have:
	\begin{equation}
		[T(u)]_\gamma = [g(1)]_\gamma = [g]_\alpha^\gamma = [Tf]_\alpha^\gamma = \coordinate{T}_\beta^\gamma [f]_\alpha^\beta = \coordinate{T}_\beta^\gamma [f(1)]_\beta = \coordinate{T}_\beta^\gamma [u]_\beta
	\end{equation}
\end{proof}

Note: in the above proof, a vector could be treated as a linear transformation from a field to vector space.

\begin{definition}
	Let $A$ be an $m \times n$ matrix. The mapping \cindex{$L_A$} that $L_A: F^n \rightarrow F^m$ defined by $L_A (x) = A x$ is called \cindex{left-multiplication transformation} . \qed
\end{definition}


A linear transformation is different from matrix:
\begin{enumerate}
    \item Matrix is finite dimensional, so it defines relation only in finite dimension space. A linear transformation could be of any dimension.
    \item For a transformation, its matrix representation depends on the chosen basis.
\end{enumerate}



\begin{theorem}
    \begin{equation}
        \begin{aligned}
            \begin{cases}
            [L_A]_\alpha^\beta &= A \\
            L_{\coordinate{T}_\alpha^\beta} &= T
        \end{cases}    
        \end{aligned}
    \end{equation}
\end{theorem}


\subsection{Inverse}

\begin{definition}
	Let $T: V\rightarrow W$ and $\mathrm{U}:W \rightarrow V$ be linear. $\mathrm{U}$ is an \cindex{inverse}   of $T$ if $\mathrm{TU} = I_W$ and $\mathrm{UT} = I_V$. If $T$ has an inverse, $T$ is \cindex{invertable}  , which is denoted as $T^{-1}$.
\end{definition}

\begin{theorem}
    $(\mathrm{UT})^{-1} = T^{-1} \mathrm{U}^{-1}$.
\end{theorem}

\begin{definition}
	Let $A$ be $n \times n$ matrix. $A$ is invertable if there is an $n \times n$ matrix $B$ that $AB=BA=I$.
\end{definition}

\begin{theorem}
    if $T$ is invertible, 
	\begin{equation*}
		\coordinate{T^{-1}}_\gamma^\beta = \left(\coordinate{T}_\beta^\gamma \right)^{-1}
	\end{equation*}
\end{theorem}
\begin{proof}
	\begin{equation*}
		I_n = \coordinate{I_V}_\beta = \coordinate{T^{-1} T}_\beta = \coordinate{T^{-1}}_\gamma^\beta \coordinate{T}_\beta^\gamma
	\end{equation*}
\end{proof}

\begin{definition}
	$V$ is \cindex{isomorphic} to $W$ if there exists a linear transformation $T:V\rightarrow W$ that is invertible. $T$ is called an \cindex{isomorphism}  from $V$ to $W$.
\end{definition}

\begin{theorem}
	$V$ is isomorphic to $W$ if $\dimension{V} = \dimension{W}$.
\end{theorem}

\begin{proof}
	If the dimensions are the same, choose basis $\beta$ of $V$ and $\gamma$ of $W$ and create a linear mapping $T:\beta \rightarrow \gamma$ by \thmref{uniquelineartransformation}.
\end{proof}


\begin{theorem}
	Let $V$ be a vector space over $F$. Then $V$ is isomorphic to $F^n$ $\Leftrightarrow$ $\dimension{V} = n$.
\end{theorem}


\begin{theorem}
	The function $\Phi: \mathcal{L}(V,M) \rightarrow M_{m \times n}(F)$ defined by $\Phi (T) = \coordinate{T}_\beta^\gamma$, is an isomorphism. The dimension has relation that 
	\begin{equation}
		\dimension{\mathcal{L}(V,M)} = \dimension{V} \times \dimension{W}
	\end{equation}
\end{theorem}


\subsection{Change of Coordinate Matrix}


\begin{theorem}
	Let $\beta$ and $\beta^\prime$ be two ordered basis of $V$. Let $Q = \coordinate{I_V}_{\beta^\prime}^\beta$, then
	\begin{enumerate}
		\item $Q$ is invertible.
		\item $\forall \alpha \in V$, $\coordinate{\alpha}_\beta = Q \coordinate{\alpha}_{\beta^\prime} = \coordinate{I_V}_{\beta^\prime}^\beta \coordinate{\alpha}_{\beta^\prime}$.
	\end{enumerate}
	
	$Q= \coordinate{I_V}_{\beta^\prime}^\beta$ is called \cindex{change of coordinate matrix} that changes from $\beta^\prime$-coordinates to $\beta$-coordinates.
\end{theorem}

\begin{proof}
    $\forall \alpha \in V $,  $\coordinate{\alpha}_\beta = \coordinate{I_V (\alpha)}_\beta =  \coordinate{I_V}_{\beta^\prime}^\beta \coordinate{\alpha}_{\beta^\prime} = Q \coordinate{\alpha}_{\beta^\prime}$.
\end{proof}

If $Q$ changes $\beta^\prime$-coordinate into $\beta$-coordinate, $Q^{-1}$ changes $\beta$-coordinate into $\beta^\prime$-coordinate.

\begin{definition}
	A \cindex{linear operator} is a linear transformation that map from $V$ to itself.
\end{definition}

\begin{theorem}\label{twoindextransform}
	If $T$ is a linear operator on $V$, then
	\begin{equation}
		\coordinate{T}_{\beta^\prime} = \coordinate{I_V}_{\beta}^{\beta^\prime} \coordinate{T}_\beta \coordinate{I_V}_{\beta^\prime}^\beta= Q^{-1} \coordinate{T}_\beta Q 
	\end{equation}
\end{theorem}

\begin{proof}
    $Q \coordinate{T}_{\beta^\prime} = [I]_{\beta^\prime}^\beta \coordinate{T}_{\beta^\prime}^{\beta^\prime} = [I T]_{\beta^\prime}^\beta = [T I]_{\beta^\prime}^\beta = \coordinate{T}_\beta^\beta [I]_{\beta^\prime}^\beta = \coordinate{T}_\beta Q$.
\end{proof}

\begin{theorem}
    Let $A \in M_{n \times n} (F)$, and $\gamma:\set{a_i}$ is an ordered basis for $F^n$. Then $\coordinate{L_A}_\gamma = Q^{-1} A Q$, where $Q = \coordinate{a_1, a_2, \dots, a_n}$.
\end{theorem}

\begin{proof}
    $\coordinate{L_A}_I = A$, so
    \begin{equation*}
        [L_A]_\gamma = \coordinate{I_V}_I^\gamma \times \coordinate{L_A}_I \times \coordinate{I_V}_\gamma^I = \coordinate{I_V}_I^\gamma \times A \times \coordinate{I_V}_\gamma^I
    \end{equation*}
    
    A take aways is that $Q$ is the change of coordinate matrix from $\gamma$ to $I$.
\end{proof}


\begin{theorem} \label{specialchangeofcoordinates}
	Let $T:V\rightarrow W$, $\beta$ and $\beta^\prime$ are ordered basis of $V$, $\gamma$ and $\gamma^\prime$ are ordered basis of $W$. Then
	\begin{equation}
		\coordinate{T}_{\beta^\prime}^{\gamma^\prime} = \coordinate{I_W}_\gamma^{\gamma^\prime} \coordinate{T}_\beta^\gamma \coordinate{I_V}_{\beta^\prime}^\beta
	\end{equation}
\end{theorem}


\begin{example}
There is an example of the usage of change of coordinate matrix: do reflection operation $T$ against a line $y = a x$. Let $\beta$ be the standard basis of $R^2$ and $\beta^\prime$ be the standard basis of $R^2$ after the rotation of $y = a x$. The operation $T$ has a matrix representation in $\beta^\prime$
	\begin{equation*}
		\coordinate{T}_{\beta^\prime} = \begin{bmatrix}
			1 & 0 \\
			0 & -1
		\end{bmatrix}		
	\end{equation*}
	Then calculate $\coordinate{T}_\beta$ based on $\coordinate{T}_{\beta^\prime}$.    
\end{example}

	



\begin{definition}
	$B$ is \cindex{similar} to $A$ if there is an invertible matrix $Q$ that $B = Q^{-1} A Q$.
\end{definition}

\begin{theorem}
If $T$ is a linear operator on finite dimension vector space $V$, and if $\beta$ and $\beta^\prime$ are any ordered basis of $V$, then $\coordinate{T}_{\beta^\prime}$ is similar to $\coordinate{T}_\beta$.    
\end{theorem}




\subsection{Quotient Space}

\begin{definition}
    Let subspace $U \subset V$, The \cindex{affine subset}  $v + U$ of $V$ is defined as:
    \begin{equation}
        v + U = \{ v + u: u \in U\}
    \end{equation}    
\end{definition}

\begin{definition}
    Let subspace $U \subset V$. Then the \cindex{quotient space} $V/U$ is defined as:
    \begin{equation}
        V/U = \{ v + U: v \in V \}
    \end{equation}
\end{definition}

\begin{definition}
    Let subspace $U \subset V$. The \cindex{quotient map} $\pi: V \rightarrow V/U$ is defined as:
    \begin{equation}
        \pi(v) = v + U
    \end{equation}
\end{definition}

\begin{theorem}
    \begin{equation}
        \dimension{V/U} = \dimension{V} - \dimension{U}
    \end{equation}
\end{theorem}

\begin{proof}
    Define $\pi : V \rightarrow V/U$. The null space is $U$.
\end{proof}

\begin{theorem}
    Define $\tilde{T}: V/\nullspace{T} \rightarrow W$ by:
    \begin{equation*}
        \tilde{T}\left(v + \nullspace{T} \right) = Tv
    \end{equation*}
    Then $\tilde{T}$ is an isomorphism between $V/\nullspace{T}$ and $T$.
\end{theorem}
\begin{proof}
    If $u + \nullspace{T} = v + \nullspace{T}$, then $u - v \in \nullspace{T}$. So $T(u - v) = T(u) - T(v) = 0$ and $T(u) = T(v)$.
\end{proof}



% dual space section
\subsection{Dual Space}


\begin{definition}
	A \cindex{linear functional} is a linear transformation that map from $V$ into $F$.
\end{definition}

\begin{definition}
	An $i$-th coordinate function $f_i$ with respect to basis $\beta$ is defined as $f_i(x) = a_i$ where
	\begin{equation*}
		\coordinate{x}_\beta = \begin{bmatrix}
			a_1 \\
			a_2 \\
			\vdots \\
			a_n
		\end{bmatrix} = \begin{bmatrix}
			f_1(a) \\
			f_2(a) \\
			\vdots \\
			f_n(a)
		\end{bmatrix}
	\end{equation*}
\end{definition}



\begin{definition}
	The \cindex{dual space}\label{dualspacedefinition} of $V$ is the vector space $V^* = \mathcal{L}(V,F)$. The \cindex{double dual space} $V^{**}$ is the dual space of $V^*$.
\end{definition}


The dimension of dual space is $\dimension{V^*} = \dimension{\mathcal{L}(V,F)} = \dimension{V} \times \dimension{F} = \dimension{V}$.

\begin{definition}
	Let $\beta = \set{x_i}$ be an ordered basis for finite dimensional vector space $V$. Define $f_i (x) = a_i$ where
	\begin{equation*}
	    \coordinate{x}_\beta = \begin{bmatrix}
	        a_1 \\
	        a_2 \\
	        \vdots \\
	        a_n
	    \end{bmatrix}
	\end{equation*}
	
	$f_i$ is the $i$-th coordinate function with respect to basis $\beta$. let $\beta^*=\set{f_i}$. Then $\beta^*$ is an ordered basis for $V^*$, and $\forall f \in V^*$, we have
	\begin{equation}
		f = \sum_{i=1}^n f(x_i) f_i
	\end{equation}
	$\beta^*$ is called the \cindex{dual basis} of $\beta$.
\end{definition}
\begin{proof}
	Let $g =\displaystyle \sum_{i=1}^n f(x_i) f_i$, we have
	\begin{equation*}
	g(x_j) = \left( \sum_{i=1}^n f(x_i) f_i \right) (x_j) = \sum_{i=1}^n f(x_i) f_i (x_j) = \sum_{i=1}^n f(x_i) \delta_{ij} =f(x_j)
	\end{equation*}
\end{proof}


\begin{theorem}
	Let $V$ and $W$ be vector space over $F$ with ordered basis $\beta$ and $\gamma$. For any linear transformation $T:V \rightarrow W$, the mapping $T^t: W^* \rightarrow V^*$ defined as $T^\top (g) = gT, \forall g \in W^*$ is a linear transformation with property that $\left[T^\top \right]_{\gamma^*}^{\beta^*} = \left(\left[T \right]_\beta^\gamma \right)^\top$.
\end{theorem}
\begin{proof}
	Let $\beta = \{x_i\}$ and $\gamma=\{y_i\}$ with dual basis $\beta^*=\{f_i\}$ and $\gamma^*=\{g_i\}$, $A=\coordinate{T}_\beta^\gamma$. we have
	\begin{equation*}
		T^\top (g_j) = g_j T = \sum_{s=1}^n (g_j T) (x_s) f_s
	\end{equation*}
	
	So the row $i$, column $j$ entry of $[T^\top]_{\gamma^*}^{\beta^*}$ is
	\begin{equation*}
	(g_j T)(x_i) = g_j (T(x_i))= g_j \left( \sum_{k=1}^m A_{kj} y_k \right) = \sum_{k=1}^m A_{kj} g_j(y_k)= \sum_{k=1}^m A_{kj} \delta_{kj} = A_{ji}
	\end{equation*}
	
	Hence $\left[T^\top \right]_{\gamma^*}^{\beta^*} = A^\top $.
\end{proof}

\begin{definition}
    For $U \subset V$, the \cindex{annihilator} of $U$, denoted as $U^0_V$, is defined as
    \begin{equation*}
        U^0_V = \{ \phi \in V^*: \phi(u) = 0, \forall u \in U \}
    \end{equation*}
    So the annihilator map $U$ to $0$. For vectors in $V - U$, the mapping could be any result. The annihilator is a subspace.
\end{definition}

\begin{theorem}
    \begin{equation}
        \dimension{U} + \dimension{U_V^0} = \dimension{V}
    \end{equation}
\end{theorem}

\begin{proof}
    Define $i \in \mathcal{L}(U,V)$ that $i(u) = u, \forall u \in U$. $i^* \in \mathcal{L}(V^*,U^*)$. So
    \begin{equation*}
        \dimension{\rangespace{i^*} } +\dimension{\nullspace{i^*}} = \dimension{V^*}
    \end{equation*}
    By definition, $\nullspace{i^*} = U^0_V$. Also $\rangespace{i^*} = U^*$.
\end{proof}


\begin{theorem}
    Let $V$ and $W$ be two finite-dimentional vector space, and $T \in \mathcal{L}(V,W)$. Then:
    \begin{enumerate}
        \item $\nullspace{T^*}  = (\rangespace{T} )^0$
        \item $\rangespace{T^*} = (\nullspace{T} )^0$
        \item $\dimension{\rangespace{T^*}} = \dimension{\text{range } T}$
        \item $\dimension{\nullspace{T^*}} = \dimension{\nullspace{T}} + \dimension{W} - \dimension{V}$
    \end{enumerate}
\end{theorem}

\begin{proof}
    Suppose $\varphi \in \text{null } T^*$. Then $ 0 = T^*(\varphi) = \varphi T$. Then
    \begin{equation*}
        0 = (\varphi T)(v) = \varphi (Tv) 
    \end{equation*}
    So $\varphi \in (\text{range } T)^0_W$.
    
    \begin{equation*}
        \begin{aligned}
            \dimension{\rangespace{T^*}} &= \dimension{W^*} - \dimension{\nullspace{T^*}} \\
            &= \dimension{W} - \dimension{\rangespace{T}^0} \\
            &= \dimension{\rangespace{T}}
        \end{aligned}
    \end{equation*}
    
    \begin{equation*}
        \begin{aligned}
            \dimension{\nullspace{T^*}} &= \dimension{\rangespace{T}^0}\\
            &= \dimension{W} - \dimension{\rangespace{T}} \\
            &= \dimension{W} - (\dimension{V} - \dimension{\nullspace{T}} ) \\
            &= \dimension{W} + \dimension{\nullspace{T}} - \dimension{V}
        \end{aligned}
    \end{equation*}
\end{proof}


\begin{definition}
    For vector $x \in V$, define $\hat{x}: V^* \rightarrow F $ by $\hat{x}(f) = f(x)$. $\hat{x}$ is a linear functional on $V^*$, so $\hat{x} \in V^{**}$.
\end{definition}


\begin{theorem}
    Define $\psi : V \rightarrow V^{**}$ by $\psi (x) = \hat{X}$.  Then $\psi$ is an isomorphism.
\end{theorem}

\begin{theorem}
    Let $V$ be a finite dimension vector space with dual space $V^*$. Every ordered basis for $V^*$ is the dual basis for some basis for $V$.
\end{theorem}

\begin{center}
    \begin{tikzcd}
V \arrow[rrddd, "V^*"{name=U}] \arrow[rrrr, "T"] && & & W \arrow[llddd, "W^*"'{name=W}] \\
\\
\\
&& F
\arrow[rightarrow, from=W, to=U, "T^\top"]
\end{tikzcd}
\end{center}





