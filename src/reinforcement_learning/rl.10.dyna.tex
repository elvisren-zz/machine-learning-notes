\section{Unify Planning and Learning}

\subsection{Model and Planning}
Planning and learning share two basic ideas:

\begin{enumerate}
	\item all involve computing value functions
	\item compute value function by update or backup operation to simulated experiences
\end{enumerate}

The difference is that planning use simulated experience generated by model, while learning use real experience generated by environment.

A \cindex{model} $\mathcal{M} = \langle \mathcal{P}, \mathcal{R} \rangle$ represents state and reward transition:
\begin{equation}
	\begin{aligned}
		S_{t+1} &\sim \mathcal{P}(S_{t+1}|S_t,A_t) \\
		R_{t+1} &= \mathcal{R}(R_{t+1}|S_t,A_t)
	\end{aligned}
\end{equation}

typically there is an assumption that $S_{t}$ and $R_t$ are independent:
\begin{equation}
	\mathbb{P}[S_{t+1},R_{t+1}|S_t,A_t] = \mathbb{P}[S_{t+1} |S_t,A_t] \times \mathbb{P}[ R_{t+1}|S_t,A_t]
\end{equation}

the problem with model based learning is that it has two source of approximation.


The learning of model $\mathcal{M}$ is a supervised learning:
\begin{equation}
	\begin{aligned}
		S_1, A_1 &\rightarrow R_2, S_2\\
		S_2, A_2 &\rightarrow R_3, S_3\\
		&\dots \\
		S_{T-1}, A_{T-1} &\rightarrow R_T, S_T
	\end{aligned}
\end{equation}

The step of planning with model:
\begin{enumerate}
	\item use supervised learning to learn a model $\mathcal{M}$
	\item use model only to generate samples $S$ and $R$
	\item apply model-free reinforcement learning to samples
\end{enumerate}

sample based planning is often more efficient.



\subsection{\cindex{Dyna-Q}}

Within a planning agent, real experience has two use cases:

\begin{description}
	\item [model-learning] update model
	\item [direct reinforcement learning] improve value and policy function
\end{description}




See Algorithm (\ref{algo:dynaq}) for detail.


\begin{algorithm}
	\caption{Dyna-Q}\label{algo:dynaq}	
	
	\begin{algorithmic}[1]
		\State $Q(s,a) \gets$ random
		\State $\text{Model}(s,a) \gets$ random
		
		\Statex
		
		\Loop
			\State $S \gets$ current state (non-terminal)
			\State $A \gets \varepsilon$-greedy$(S,Q)$
			\State take $A$, record $R$ and $S\prime$
			\State \Comment direct \cindex{Q-learning}
			\State $Q(S,A) \gets Q(S,A) + \alpha \Big (R + \gamma \max_a Q(S',a) - Q(S,A ) \Big)$ 
			\State $\text{Model}(S,A) \gets (R,S')$ \Comment assuming deterministic environment
			\State $i \gets 0$
			\Repeat \Comment indirect RL, model learning process
				\State $S \gets $ random previously observed state
				\State $A \gets $ random action taken in state $S$
				\State $R,S' \gets \text{Model}(S,A)$
				\State $Q(S,A) \gets Q(S,A) + \alpha \Big(R + \gamma \max_a Q(S',a) - Q(S,A ) \Big) $
				\State $ i \gets i + 1$
			\Until{$i = n$}
		\EndLoop
	\end{algorithmic}
\end{algorithm}

\subsection{Prioritized Sweeping}

In background model improvement it is not useful to sweep over all states. The state and transition that leads to goal states, or to state whose value has changed, are more useful. It is called \cindex{backward focusing}. 

In \cindex{backward focusing} the state that has changed a lot are more likely to change. So when sweeping backward from the goal, choose the ones with the biggest change history and update them. 



See Algorithm (\ref{algo:bwfocusing}) for detail.


\begin{algorithm}
	\caption{prioritized sweeping with Dyna-Q}\label{algo:bwfocusing}	
	
	\begin{algorithmic}[1]
		\State $Q(s,a) \gets$ random
		\State $\text{Model}(s,a) \gets$ random
		\State $\text{PQueue} \gets []$
		
		\Statex
		
		\Loop
			\State $S \gets$ current state (non-terminal)
			\State $A \gets \text{policy}(S,Q)$
			\State take $A$, record $R$ and $S\prime$
			\State $P \gets |R + \gamma \underset{a}{\max}\ Q(S',a) - Q(S,A)|$
			\State $Model(S,A) \gets (R,S')$ \Comment assuming deterministic environment
			\State $i \gets 0$
			\Repeat \Comment indirect RL, model learning process
				\State $\langle S, A\rangle \gets \text{PQueue.head}()$
				\State $R,S' \gets \text{Model}(S,A)$
				\State $Q(S,A) \gets Q(S,A) + \alpha \Big(R + \gamma \underset{a}{\max}\ Q(S',a) - Q(S,A ) \Big) $
				
				\For{$\forall \langle \overline{S}, \overline{A}\rangle $ that leads to $S$}
					\State $\overline{R} \gets$ predicted reward from $\overline{S}, \overline{A}, S$
					\State $P \gets |\overline{R} + \gamma \underset{a}{\max}\ Q(S,a) - Q(\overline{S}, \overline{A})|$
					\If{$P > \theta$}
						\State $\text{PQueue.add}(P \rightarrow \langle \overline{S}, \overline{A} \rangle )$
					\EndIf
				\EndFor
			\Until{$i = n$}
		\EndLoop
	\end{algorithmic}
\end{algorithm}

Extension to stochastic environment is done by updating the $P$ with sampled expected value.



\subsection{Expected and Sample Update}

DP uses \cindex{expected} update which consider all possible events while TD uses \cindex{sample} update which consider a single example.

\cindex{expected} update is better because it can avoid sampling error. But the expectation calculation is expensive, which is roughly the time of \cindex{branching factor} times of sample update. In practice the \cindex{branching factor} is usually very high and \cindex{sample} update is preferred.

When computation power is limited, it is a choice between  sample update for many $\langle S,A \rangle$ pair and expectation update for some  $\langle S,A \rangle$. In practice the sample update error will drop along the curve $\sqrt \frac{b-b}{bt}$ where $b$ is the \cindex{branching factor} and $t$ is the number of performed sample update. So the converge rate is very fast by taking sample update.




