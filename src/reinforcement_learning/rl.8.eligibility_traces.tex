\section{Eligibility Traces}

\subsection{$\lambda$-return}
\cindex{eligibility trace} is a short term memory vector that has the same dimension as $\mathbf{W}$. 

The \cindex{forward view of TD($\lambda$)} is defined as:
\begin{equation}
	\begin{cases}
		G_t^\lambda = (1-\lambda)\sum\limits_{n=1}^\infty \lambda^{n-1} G_{t:t+n} & \text{, for continuous task}\\
		\\
		G_t^\lambda = (1-\lambda)\sum\limits_{n=1}^{T-t-1} \lambda^{n-1} G_{t:t+n} + \lambda^{T-t-1}G_t & \text{, for episodes} 
	\end{cases}
\end{equation}

\subsection{TD($\lambda$)}

In \cindex{backward view of TD($\lambda$)}, the \cindex{eligibility trace} $z_t \in \mathbb{R}^d$ is defined as:
\begin{equation}
	\begin{aligned}
		z_{-1} &= 0 \\
		z_t &= \gamma \lambda z_{t-1} + \nabla \widehat{v} (S_t, \mathbf{W}_t)
	\end{aligned}
\end{equation}

Of which $\gamma$ is the discount rate and $\lambda$ is the $\lambda$ in TD($\lambda$).

The TD error is defined as:
\begin{equation}
	\delta_t = R_{t+1} + \gamma \widehat{v}(S_{t+1}, \mathbf{W}_t) -\widehat{v}(S_t, \mathbf{W}_t) 
\end{equation}

The gradient is defined as:
\begin{equation}
	\mathbf{W}_{t+1} = \mathbf{W}_t + \alpha \delta_t z_t
\end{equation}

Here the $\alpha$ is the ratio used for mean value converge calculation. 

If $\lambda = 0$, TD($\lambda$) becomes one-step semi-gradient TD. 

If $\lambda = 1$, TD($\lambda$) becomes Monte Carlo calculation.


Linear TD($\lambda$) will converge in on-policy case if step size follows formula (\ref{convergenceofsequence}):


\begin{equation}\label{semigradientlrerror}
	\overline{\text{VE}}(\mathbf{W}_{\infty}) \leq \frac{1 - \gamma \lambda}{1-\gamma} \underset{\mathbf{W}}{\min}\ \overline{\text{VE}}(\mathbf{W})
\end{equation}

In practice, do not choose $\lambda = 1$ which is the poorest choice.











